{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In this section we will implement a linear regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "1. Implement a simple forward model: $y = W x + b$\n",
    "\n",
    "1. build a `predict` function which returns the predicted regression value given an input $x$\n",
    "\n",
    "1. build an `accuracy` function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$ (for regression we typically use Mean Squared Error (MSE) as metric)\n",
    "\n",
    "1. build a `grad` function which computes the gradients for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well defined\n",
    "\n",
    "1. build a `train` function which uses the `grad` function output to update $W$ and $b$\n",
    "\n",
    "You should be able to reuse a lot of the parts from `01-loss.ipynb` for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our toy data for this task\n",
    "X = np.random.uniform(0, 10, size=20)\n",
    "temp = 1.3*X + 15 + np.random.normal(0, 1, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, temp, 'o')\n",
    "plt.xlabel(\"Minutes of sunshine in the last 10minutes\")\n",
    "plt.ylabel(\"Temperature [C]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self):\n",
    "        self.W = np.random.uniform(high=0.5, low=-0.5)\n",
    "        self.b = np.random.uniform(high=0.5, low=-0.5)\n",
    "        print(self.W)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: for each sample in X return the predicted value, X is a vector!\n",
    "        # Note: through out the week \"TODO:\" means something for you to do, and\n",
    "        # `return None` or `return blah` means the function should return something\n",
    "        # the None is just a placeholder\n",
    "        print(\"value of X\", X)\n",
    "        y_pred = None\n",
    "        return y_pred\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO?: compute gradient with respect to W and b for one sample x\n",
    "        # and the true value y_true\n",
    "        #grad_W = ...\n",
    "        #grad_b = ...\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO: compute one step of the gradient descent update, update W and b\n",
    "        grads = self.grad_loss(x, y)\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # TODO: compute the loss for the sample x with true value y\n",
    "        return None\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        # TODO: compute accuracy for samples X with true values y\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Code below here won't run until after you add to the class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* how do you know that you trained for enough epochs?\n",
    "* visualise how the loss changes over the epochs\n",
    "* are more epochs always better? How could you show this?\n",
    "* change the setup to use stochastic gradient descent\n",
    "* (bonus) visualise the values of W and b over the epochs\n",
    "* (bonus) can you see a difference for the paths of W and b between SGD and normal GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "print('initial value of W: %.4f and b: %.4f' % (lr.W, lr.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for x_, y_ in zip(x, temp):\n",
    "    lr.train(x_, y_, learning_rate)\n",
    "    train_acc = lr.accuracy(x, temp)\n",
    "print(\"Update: train accuracy: %0.3f\"% (train_acc), end=' ')\n",
    "print('value of W: %.4f and b: %.4f' % (lr.W, lr.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.linspace(0, 10, 100)\n",
    "plt.plot(x, temp, 'o')\n",
    "plt.plot(line, lr.predict(line));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
