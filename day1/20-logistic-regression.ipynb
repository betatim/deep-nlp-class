{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "1. Implement a simple forward model with no hidden layer (equivalent to logistic regression):\n",
    "$y = softmax(\\mathbf{W} x + b)$\n",
    "\n",
    "1. build a `predict` function which returns the most probable class given an input $x$\n",
    "\n",
    "1. build an `accuracy` function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "1. understand the method computing $\\frac{d}{dW} -\\log(softmax(Wx + b))$ for an $x$ and its corresponding expected output $y_{true}$\n",
    "\n",
    "1. build a `train` function which uses the `grad` function output to update $\\mathbf{W}$ and $b$\n",
    "\n",
    "In this exercise we will take small images of hand written digits and classify them. This is a multi-class classification problem. We will use Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample our model will output ten values. The probability it assigns for the image being in each of the ten classes. They should sum to one.\n",
    "\n",
    "We express our desired output, the ground truth in a similar fashion by one-hot encoding it. So the class 3 will be represented as a vector `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`.\n",
    "\n",
    "First let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(10, [3,2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(10, [0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to take a look at the dataset before we start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45 # change this to see different examples\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"true image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- normalization (for more take a look at http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- train/test split\n",
    "\n",
    "Why the need for preprocessing inputs? For neural networks it is important to have the min and max of all features on roughly the same scale. You do not want one feature from 0.0001 to 0.001 and another from 10 - 100. The result would be (amongst others) that the gradient involving one feature would be much larger than in the other direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# Important to only use training data to learn the\n",
    "# feature scaling. Training data simulates the future\n",
    "# so we should not access any information from it.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_train = one_hot(10, y_train)\n",
    "Y_test = one_hot(10, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = one_hot(10, y_train)\n",
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display one of the transformed samples (after feature standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"transformed sample\\n(standardised)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler object makes it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(scaler.inverse_transform(X_train[sample_index]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO: implement me\n",
    "    # the pass keyword is a Python placeholder, delete it\n",
    "    # once you have some code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a naive implementation of softmax might not be able process a batch of activations in a single call (but we need that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))\n",
    "# check the keepdims argument of `np.sum` for a hint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "This is a multi-class classification problem. The loss function we will use unfortunately has many names that are used interchangably. You can also find many simplified definitions for the two class case that do not note that they are simplifications. For example [wikipedia](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression) has a nice worked example but only a small note pointing out that this is for two classes only. Maybe read this first, then try and extend it to multiple classes using the notation from the beginning of that wikipedia entry.\n",
    "\n",
    "We will use \"log loss\", \"cross entropy\", or even \"categorical crossentropy\".\n",
    "\n",
    "The general expression for the log loss of **one sample** is:\n",
    "$$\n",
    "\\ell = - \\sum_k \\mathbf{1}_{k=y} log(p_k)\n",
    "$$\n",
    "\n",
    "For a problem with $k$ classes, where $y$ denotes the true class, and $p_k$ the probability the network predicts for class $k$. $\\mathbf{1}$ is the indicator function, a clever way of saying \"zero if `k!=y`, one otherwise\". The loss over a complete training (or testing or validation) set of samples is the sum(!) of $\\ell$ for each sample.\n",
    "\n",
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood. For this problem we are using the \"log loss\" loss function (also known as cross-entropy).\n",
    "\n",
    "This `nll` is a scalar computed using all samples in our training set. The likelihood measures how likely something is. Using the term \"likelihood\" instead of probability because the likelihood does not have to be a probability.\n",
    "\n",
    "$$L = P_1 \\cdot P_2 \\cdot P_3 \\cdot ... $$\n",
    "\n",
    "However multiplying lots of small numbers together is a numerical disaster. Use $\\log(ab) = \\log(a) + log(b)$ to convert the product into a sum (hence the name log-likelihood). The negative just means we multiply everything by minus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 1e-8 # this is here to give you a hint on how to deal with the case when y_pred=0\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `nll` of a very confident yet incorrect prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be passed in as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have all the ingredients for training a logistic regression model using gradient descent.\n",
    "\n",
    "Let's study it one sample at a time. To help understand how to implement the various methods here do read on to see how it will be used. Then start implementing things that you can test straight away. For example the forward pass can be tested before trying to train the model. You can also check the computation of the accuracy with a randomly initialised model as you know what its accuracy should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO: compute normalised scores for each class, this is the output\n",
    "        # of the softmax()\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: for each sample in X return the predicted class\n",
    "        # translate the class probabilities of each sample into a human\n",
    "        # friendly representation using integers\n",
    "        pass\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO: compute gradient with respect to W and b for a sample x\n",
    "        # and the true label y_true\n",
    "        # If you are confident in your linear algebra skills attempt to\n",
    "        # write this from scratch.\n",
    "        # Notes on how to do it: https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf\n",
    "        # In particular the last section\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        grad_W = np.outer(x, dnll_output)\n",
    "        grad_b = dnll_output\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO: compute one step of traditional gradient descent update without momentum\n",
    "        # and update W and b\n",
    "        grads = self.grad_loss(x, y)\n",
    "        # ... do something with `grads`\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # TODO: use `nll` to compute the loss for the sample x with true label y\n",
    "        pass\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        # TODO: compute accuracy for samples X with true labels y\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = Y_train.shape[1]\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_train, y_train)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))\n",
    "# Question: do you think the accuracy makes sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the untrained model on an example\n",
    "sample_idx = 3\n",
    "plt.plot(lr.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(lr.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on an example\n",
    "sample_idx = 899\n",
    "plt.plot(lr.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(lr.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* can you find examples that are mispredicted, is there a pattern to the wrong predictions?\n",
    "* visualise these samples and their predicted classes\n",
    "* plot the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to find classes that are hard to separate (maybe eight vs nine?)\n",
    "* do things improve with training for more epochs?\n",
    "* experiment with different values of the learning rate. Can you accelerate learning? What happens for very large values of the learning rate? Very small ones?\n",
    "* convert the training setup to use stochastic gradient descent, does this change things?\n",
    "* what is the optimal number of epochs to train for? Why? Plot the value of the loss (on the training and on the testing dataset) as a function of the epoch. Is there a sweet spot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
