{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# Linear model, loss, gradient descent\n",
    "\n",
    "This notebook introduces the concepts of loss and parameter optimisation by gradient descent.\n",
    "\n",
    "We will use a simple, one dimensional linear model: $y = w*x + b$. However the principles extend to any model in any number of dimensions.\n",
    "\n",
    "Our toy model assumes that there is a linear relationship between the number of minutes of sunshine in a ten minute window and the air temperature. With our (made up) historical dataset we can make predictions about the air temperature given how many minutes of sunshine there were in the last ten minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# our toy data for this task\n",
    "X = np.random.uniform(0, 10, size=20)\n",
    "y = 1.3*X + 15 + np.random.normal(0, 1, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'o')\n",
    "plt.xlabel(\"Minutes of sunshine in the last 10minutes\")\n",
    "plt.ylabel(\"Temperature [C]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the temperature we measure the number of minutes of sunshine (say 4) and then look at our historical data and estimate that the remperature will be around 21C. To deal with the noise in the observations we average nearby observations in our head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "line = np.linspace(0, 10, 100)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# use numpy to compute and plot your prediction for all values in `line`\n",
    "# write your model so that you can later change the parameters\n",
    "# `w` (the slope) and `b` (the offset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your `matplotlib` knowledge is a bit rusty checkout their gallery: https://matplotlib.org/gallery.html# Chances are you can find a plot very similar to what you want to do there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## Regression vs Classification\n",
    "\n",
    "Supervised learning can be split into two types: classification and regression. What kind of problem is this? Why?\n",
    "\n",
    "## Loss\n",
    "\n",
    "In one dimension, with a linear problem we humans do a very good job of\n",
    "fitting a model to the data. How do we do it? We try and find a line that\n",
    "has roughly the same number of points above it as below it. This way we\n",
    "minimise the total distance between all the points and the line.\n",
    "\n",
    "To make that intuition quantitative we use a loss function. A popular loss\n",
    "function for regression problems is the squared loss: $ \\ell(y, x) = (y - prediction(x))^2 $. This measures how close our prediction at $x$ is to the true value $y$ taken from our historical dataset.\n",
    "\n",
    "To get the loss for the whole dataset we sum the loss for each point $\\ell(y, x)$ over all samples. This is called the Mean square error (MSE), it is the average squared loss per sample over the whole dataset.\n",
    "\n",
    "$$\n",
    "L(D, w, b) = MSE = \\frac{1}{N} \\sum_{(x,y) \\in D} (y - prediction(x))^2\n",
    "$$\n",
    "\n",
    "where $D$ is our training dataset and $N$ is the total number of samples in $D$.\n",
    "\n",
    "Our $prediction(x)$ will depend on the parameters of the model. In our case $w$ and $b$. This means we can now judge how well each set of parameters is doing and pick the one with the smallest loss.\n",
    "\n",
    "The total loss $L$ depends on $D$ and the values of our parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# write a function that computes the loss for a given set of\n",
    "# parameters and training dataset\n",
    "# plot the loss as a function of `w` for a fixed value of `b=15`\n",
    "# can you identify the best fit value of `b`? Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## Parameter updates\n",
    "\n",
    "We now have a way to compute how good a set of parameters is. Next we need a way to improve a set of parameters. For a new problem we usually start with a random guess of the value of the parameters and then need to iteratively improve them.\n",
    "\n",
    "For each set of parameters we can compute the derivative of the loss at that point. We take the partial derivative of $L$ with respect to $w$ and $b$. This allows us to update our parameters by taking a step \"downhill\".\n",
    "\n",
    "To compute the next best value of $w$ we update it according to:\n",
    "$$\n",
    "w_{new} = w + \\alpha \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "The parameter $\\alpha$ is called the learning rate, it sets how big a step we take.\n",
    "\n",
    "Experiment with different values of the learning rate: [on the tensorflow playground](https://rawgit.com/tensorflow/playground/gh-pages/index.html?hl=en#activation=linear&batchSize=3&dataset=gauss&regDataset=reg-plane&learningRate=3&regularizationRate=0&noise=80&networkShape=&seed=0.245&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&tutorial=dp-reducing-loss-learning-rate&problem=classification&initZero=false&hideText=true&numHiddenLayers_hide=true&playButton_hide=true&dataset_hide=true&percTrainData_hide=true&noise_hide=true&batchSize_hide=true&xTimesY_hide=true&xSquared_hide=true&ySquared_hide=true&sinX_hide=true&sinY_hide=true&activation_hide=true&learningRate_hide=false&regularization_hide=true&regularizationRate_hide=true&problem_hide=true). The playground shows a two dimensional classification problem. Orange and blue dots. The goal is to find a good boundary to separate the two classes.\n",
    "\n",
    "### Conceptual questions\n",
    "\n",
    "The given learning rate - 3 - is pretty high. Observe how that high learning rate affects your model by clicking the \"Step\" button 10 or 20 times. After each iteration, look at how the model visualisation changes dramatically. You might even see some instability after the model appears to have converged.\n",
    "\n",
    "Press the reset button. What happens when you reduce the learning rate to\n",
    "a much smaller value? How many more steps does it take to reach convergence? Is there an optimal value for the learning rate?\n",
    "\n",
    "\n",
    "### Coding\n",
    "Next we will implement a simple gradient descent optimiser. The goal is to find the best values of `w` and `b` according to the loss function we are using (mean-squared error).\n",
    "\n",
    "Feel free to use Wolfram alpha or similar to check you got the correct expression for the\n",
    "partial derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# write the code to compute the gradient of the loss with respect to `w`\n",
    "# and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# write a for loop to perform ten iterations of gradient descent\n",
    "# optimising both w and b starting from random values.\n",
    "# What is one iteration? An alternative name is epoch. It is\n",
    "# one complete loop through the training data.\n",
    "# Classic optimisers will loop through whole dataset to compute\n",
    "# the gradient before updating the parameters. This is wasteful\n",
    "# if you have 10s or 100s of thousands of samples. You can get\n",
    "# an estimate of the gradient with just a few hundred examples, or\n",
    "# even just one. This is called stochastic gradient descent or\n",
    "# stochastic mini-batch gradient descent.\n",
    "#\n",
    "# We don't have a lot of data, but let's still update the parameters\n",
    "# after looking at each sample. If you have time implement mini-batches.\n",
    "#\n",
    "# To help debug start with one of the parameters set to its true\n",
    "# value and only optimise the second one. Check the sign of the gradient.\n",
    "# \n",
    "# Things to do once you have it working: plot the loss at each\n",
    "# step/iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
