{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Translating\" numbers with seq2seq\n",
    "\n",
    "In the following we will try to build a translation model from german phrases describing numbers to the corresponding numeric representation (base 10).\n",
    "\n",
    "This is a toy machine translation task with a very restricted vocabulary and a single valid translation for each source phrase which makes it possible to train on a laptop computer and easier to evaluate. Despite those limitations we expect that this task will highlight interesting properties of Seq2Seq models including:\n",
    "\n",
    "- the ability to deal with different length of the source and target sequences,\n",
    "- basic counting and \"reasoning\" capabilities of LSTM and GRU models.\n",
    "\n",
    "The parallel text data is generated from a \"ground-truth\" Python function named `to_german` in the `zahlen` module. It seems to generate reasonably good phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    21 ein und zwanzig\n",
      "    42 zwei und vierzig\n",
      "    80 achtzig\n",
      "    81 ein und achtzig\n",
      "    88 acht und achtzig\n",
      "   300 dreihundert\n",
      "   213 zweihundert und dreizehn\n",
      "  1100 eintausend einhundert\n",
      "  1201 eintausend zweihundert und eins\n",
      "301000 dreihundert und ein tausend\n",
      " 80080 achtzig tausend und achtzig\n"
     ]
    }
   ],
   "source": [
    "from zahlen import to_german\n",
    "\n",
    "for x in [21, 42, 80, 81, 88, 300, 213, 1100, 1201, 301000, 80080]:\n",
    "    print(str(x).rjust(6), to_german(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Training Set\n",
    "\n",
    "The following will generate 20000 example phrases for numbers between 1 and 1,000,000. We chose to over-represent small numbers by generating all the possible short sequences between `1` and `exhaustive=5000`.\n",
    "\n",
    "We then split the generated set into non-overlapping train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from zahlen import generate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "numbers, german_numbers = generate(\n",
    "    20000, low=1, high=int(1e6), exhaustive=5000, seed=0)\n",
    "num_train, num_dev, de_train, de_dev = train_test_split(\n",
    "    numbers, german_numbers, test_size=0.5, random_state=0)\n",
    "\n",
    "num_val, num_test, de_val, de_test = train_test_split(\n",
    "    num_dev, de_dev, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000, 5000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_train), len(de_val), len(de_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197906 einhundert sieben und neunzig tausend neunhundert und sechs\n",
      "660886 sechshundert und sechzig tausend achthundert sechs und achtzig\n",
      "  2746 zweitausend siebenhundert sechs und vierzig\n",
      "165370 einhundert fuenf und sechzig tausend dreihundert und siebzig\n",
      "870470 achthundert und siebzig tausend vierhundert und siebzig\n"
     ]
    }
   ],
   "source": [
    "for i, de_phrase, num_phrase in zip(range(5), de_train, num_train):\n",
    "    print(num_phrase.rjust(6), de_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124590 einhundert vier und zwanzig tausend fuenfhundert und neunzig\n",
      "113044 einhundert und dreizehn tausend vier und vierzig\n",
      "449006 vierhundert neun und vierzig tausend und sechs\n",
      "723927 siebenhundert drei und zwanzig tausend neunhundert sieben und zwanzig\n",
      "292178 zweihundert zwei und neunzig tausend einhundert acht und siebzig\n"
     ]
    }
   ],
   "source": [
    "for i, de_phrase, num_phrase in zip(range(5), de_val, num_val):\n",
    "    print(num_phrase.rjust(6), de_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies\n",
    "\n",
    "Build the vocabularies from the training set only. This means there is a chance to have some out-of-vocabulary words in the validation and test sets.\n",
    "\n",
    "First we need to introduce specific symbols that will be used to:\n",
    "- pad sequences\n",
    "- mark the beginning of translation\n",
    "- mark the end of translation\n",
    "- be used as a placehold for out-of-vocabulary symbols (not seen in the training set).\n",
    "\n",
    "Here we use the same convention as the [tensorflow seq2seq tutorial](https://www.tensorflow.org/tutorials/seq2seq):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, GO, EOS, UNK = START_VOCAB = ['_PAD', '_GO', '_EOS', '_UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the vocabulary we need to tokenise the sequences of symbols. For the digital number representation we use character level tokenisation while whitespace-based word level tokenisation will do for the German phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(sentence, word_level=True):\n",
    "    if word_level:\n",
    "        return sentence.split()\n",
    "    else:\n",
    "        return [sentence[i:i + 1] for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise('1234', word_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['einhundert', 'und', 'dreizehn', 'tausend', 'vier', 'und', 'vierzig']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise('einhundert und dreizehn tausend vier und vierzig', word_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this tokenisation strategy to assign a unique integer token ID to each possible token string found in the training set in each language ('German' and 'numeric'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenised_sequences):\n",
    "    rev_vocabulary = START_VOCAB[:]\n",
    "    unique_tokens = set()\n",
    "    for tokens in tokenised_sequences:\n",
    "        unique_tokens.update(tokens)\n",
    "    rev_vocabulary += sorted(unique_tokens)\n",
    "    vocabulary = {}\n",
    "    for i, token in enumerate(rev_vocabulary):\n",
    "        vocabulary[token] = i\n",
    "    return vocabulary, rev_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_de_train = [tokenise(s, word_level=True) for s in de_train]\n",
    "tokenised_num_train = [tokenise(s, word_level=False) for s in num_train]\n",
    "\n",
    "de_vocab, rev_de_vocab = build_vocabulary(tokenised_de_train)\n",
    "num_vocab, rev_num_vocab = build_vocabulary(tokenised_num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two languages do not have the same vocabulary sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      _EOS 2\n",
      "       _GO 1\n",
      "      _PAD 0\n",
      "      _UNK 3\n",
      "      acht 4\n",
      "achthundert 5\n",
      "achttausend 6\n",
      "  achtzehn 7\n",
      "   achtzig 8\n",
      "      drei 9\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(de_vocab.items())[:10]:\n",
    "    print(k.rjust(10), v)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0 4\n",
      "         1 5\n",
      "         2 6\n",
      "         3 7\n",
      "         4 8\n",
      "         5 9\n",
      "         6 10\n",
      "         7 11\n",
      "         8 12\n",
      "         9 13\n",
      "      _EOS 2\n",
      "       _GO 1\n",
      "      _PAD 0\n",
      "      _UNK 3\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(num_vocab.items()):\n",
    "    print(k.rjust(10), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also built the reverse mappings from token ids to token string representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_PAD', '_GO', '_EOS', '_UNK', 'acht', 'achthundert', 'achttausend', 'achtzehn', 'achtzig', 'drei', 'dreihundert', 'dreissig', 'dreitausend', 'dreizehn', 'ein', 'einhundert', 'eins', 'eintausend', 'elf', 'fuenf', 'fuenfhundert', 'fuenftausend', 'fuenfzehn', 'fuenfzig', 'neun', 'neunhundert', 'neuntausend', 'neunzehn', 'neunzig', 'sechs', 'sechshundert', 'sechstausend', 'sechzehn', 'sechzig', 'sieben', 'siebenhundert', 'siebentausend', 'siebzehn', 'siebzig', 'tausend', 'und', 'vier', 'vierhundert', 'viertausend', 'vierzehn', 'vierzig', 'zehn', 'zwanzig', 'zwei', 'zweihundert', 'zweitausend', 'zwoelf']\n"
     ]
    }
   ],
   "source": [
    "print(rev_de_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_PAD', '_GO', '_EOS', '_UNK', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(rev_num_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq with a single GRU architecture\n",
    "\n",
    "<img src=\"basic_seq2seq.png\" width=\"80%\" />\n",
    "\n",
    "From: [Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" NIPS 2014](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "\n",
    "\n",
    "Next we need to prepare the inputs for training. For a given source - target sequence pair, we will:\n",
    "- tokenise the source and target sequences;\n",
    "- reverse the order of the source sequence;\n",
    "- create the input sequence by creating a list consisting of the reversed source sequence, the `_GO` token and the target sequence in the original order;\n",
    "- build the output sequence by appending the `_EOS` token to the source sequence.\n",
    "\n",
    "Let's do this as a function using the original string representations for the tokens so as to make it easier to debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Write a function that turns a pair of tokenized (source, target) sequences into a pair of (input, output) sequences as described above.\n",
    "- The function should have a `reverse_source=True` as an option so we can experiment with not reversing the source sequence.\n",
    "\n",
    "Notes: \n",
    "- The function should output two list of string tokens: one to be fed in as the input and the other as expected output for the seq2seq network.\n",
    "- Do not pad the sequences: we will handle the padding later.\n",
    "- Don't forget to insert the `_GO` and `_EOS` special symbols at the correct locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_output(source_tokens, target_tokens, reverse_source=True):\n",
    "    if reverse_source:\n",
    "        source_tokens = source_tokens[::-1]\n",
    "    input_tokens = source_tokens + [GO] + target_tokens\n",
    "    output_tokens = target_tokens + [EOS]\n",
    "    return input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['ein', 'hundert', 'und', 'eins'],\n",
    "    ['1', '2', '1'],\n",
    ")\n",
    "# Expected outputs:\n",
    "# ['eins', 'und', 'hundert', 'ein', '_GO', '1', '2', '1']\n",
    "# ['1', '2', '1', '_EOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eins', 'und', 'hundert', 'ein', '_GO', '1', '2', '1']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '1', '_EOS']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation of the parallel corpus\n",
    "\n",
    "Let's apply the previous transformation to each pair of (source, target) sequene and use a shared vocabulary to store the results in numpy arrays of integer token IDs, with padding on the left so that all input / output sequences have the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenised_sequences = tokenised_de_train + tokenised_num_train\n",
    "shared_vocab, rev_shared_vocab = build_vocabulary(all_tokenised_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(s) for s in tokenised_de_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(s) for s in tokenised_num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# found by introspection of our training set\n",
    "# max DE train length + max numeric train lenght + 1 token\n",
    "max_length = 9 + 6 + 1\n",
    "\n",
    "def vectorise_corpus(source_sequences, target_sequences, shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True,\n",
    "                     max_length=max_length):\n",
    "    assert len(source_sequences) == len(target_sequences)\n",
    "    n_sequences = len(source_sequences)\n",
    "    # a numpy array to store the source tokens\n",
    "    source_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    # set everything to represent \"PADDING\"\n",
    "    source_ids.fill(shared_vocab[PAD])\n",
    "    # place to store the target tokens\n",
    "    target_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    target_ids.fill(shared_vocab[PAD])\n",
    "\n",
    "    numbered_pairs = zip(range(n_sequences), source_sequences, target_sequences)\n",
    "    for i, source_seq, target_seq in numbered_pairs:\n",
    "        source_tokens = tokenise(source_seq, word_level=word_level_source)\n",
    "        target_tokens = tokenise(target_seq, word_level=word_level_target)\n",
    "        \n",
    "        in_tokens, out_tokens = make_input_output(source_tokens, target_tokens)\n",
    "        \n",
    "        in_token_ids = [shared_vocab.get(t, UNK) for t in in_tokens]\n",
    "        source_ids[i, -len(in_token_ids):] = in_token_ids\n",
    "    \n",
    "        out_token_ids = [shared_vocab.get(t, UNK) for t in out_tokens]\n",
    "        target_ids[i, -len(out_token_ids):] = out_token_ids\n",
    "    return source_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = vectorise_corpus(de_train, num_train, shared_vocab,\n",
    "                                    word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 39, 50, 35, 49, 38, 50, 44, 25,  1,  5, 13, 11, 13,  4, 10], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'einhundert sieben und neunzig tausend neunhundert und sechs'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'197906'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 39, 50, 35, 49, 38, 50, 44, 25,  1,  5, 13, 11, 13,  4, 10], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  5, 13, 11, 13,  4, 10,  2], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. In particular we can note:\n",
    "\n",
    "- the PAD=0 symbol at the beginning of the two sequences,\n",
    "- the input sequence has the GO=1 symbol to separate the source from the target,\n",
    "- the output sequence is a shifted version of the target and ends with EOS=2.\n",
    "\n",
    "Let's vectorise the validation and test set to be able to evaluate our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = vectorise_corpus(de_val, num_val, shared_vocab,\n",
    "                                word_level_target=False)\n",
    "X_test, Y_test = vectorise_corpus(de_test, num_test, shared_vocab,\n",
    "                                  word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 16), (5000, 16))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 16), (5000, 16))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Seq2Seq architecture\n",
    "\n",
    "To keep the architecture simple we will use the same RNN model and weights for both the encoder part (before the `_GO` token) and the decoder part (after the `_GO` token).\n",
    "\n",
    "We are using the GRU recurrent cell instead of a LSTM because it is slightly faster to compute and should give comparable results.\n",
    "\n",
    "Building a Seq2Seq model:\n",
    "  - Starting with an Embedding layer;\n",
    "  - Then a single GRU layer: the GRU layer should yield a sequence of output vectors, one at each timestep;\n",
    "  - Then a Dense layer to adapt the ouput dimension of the GRU layer to the dimension of the output vocabulary;\n",
    "  - Don't forget to insert some Dropout layer(s), especially after the Embedding layer.\n",
    "\n",
    "Note:\n",
    "- The output dimension of the Embedding layer should be smaller than usual because we have small vocabulary size;\n",
    "- The dimension of the GRU should be larger to give the Seq2Seq model enough \"working memory\" to memorize the full input sequence before decoding it;\n",
    "- Your model should output a shape `[batch, sequence_length, vocab_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "vocab_size = len(shared_vocab)\n",
    "simple_seq2seq = Sequential()\n",
    "simple_seq2seq.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "simple_seq2seq.add(Dropout(0.2))\n",
    "simple_seq2seq.add(GRU(256, return_sequences=True))\n",
    "simple_seq2seq.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# We use the sparse_categorical_crossentropy loss to be able to pass\n",
    "# integer-coded output for the token IDs without having to convert to one-hot\n",
    "# codes first\n",
    "simple_seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- What is the expected shape of the output of the model when we feed it with an input of 16 tokens? What is the meaning of the values in the output of the model?\n",
    "- What is the shape of the output of each layer in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 62)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_seq2seq.predict(X_train[0:1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 16, 32)            1984      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 16, 256)           221952    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16, 62)            15934     \n",
      "=================================================================\n",
      "Total params: 239,870\n",
      "Trainable params: 239,870\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_seq2seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register a callback to automatically snapshot the best model by measuring the performance of the model on the validation set at the end of each epoch during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "best_model_fname = \"simple_seq2seq_checkpoint.h5\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname, monitor='val_loss',\n",
    "                                save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the `np.expand_dims` trick on Y: this is required by Keras because if we use a sparse (integer-based) representation for the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/90\n",
      " - 12s - loss: 1.1229 - val_loss: 0.8007\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.80065, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 2/90\n",
      " - 11s - loss: 0.7805 - val_loss: 0.7631\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.80065 to 0.76312, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 3/90\n",
      " - 11s - loss: 0.7451 - val_loss: 0.7054\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76312 to 0.70539, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 4/90\n",
      " - 11s - loss: 0.6745 - val_loss: 0.6243\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.70539 to 0.62432, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 5/90\n",
      " - 11s - loss: 0.5989 - val_loss: 0.5345\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62432 to 0.53448, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 6/90\n",
      " - 11s - loss: 0.5119 - val_loss: 0.4515\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53448 to 0.45153, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 7/90\n",
      " - 11s - loss: 0.4250 - val_loss: 0.3934\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.45153 to 0.39341, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 8/90\n",
      " - 11s - loss: 0.3486 - val_loss: 0.2888\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39341 to 0.28881, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 9/90\n",
      " - 11s - loss: 0.2656 - val_loss: 0.1903\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.28881 to 0.19027, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 10/90\n",
      " - 11s - loss: 0.1873 - val_loss: 0.1286\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.19027 to 0.12862, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 11/90\n",
      " - 11s - loss: 0.1284 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12862 to 0.07723, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 12/90\n",
      " - 12s - loss: 0.0895 - val_loss: 0.0544\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07723 to 0.05442, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 13/90\n",
      " - 11s - loss: 0.0650 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05442 to 0.03556, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 14/90\n",
      " - 11s - loss: 0.0486 - val_loss: 0.0288\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03556 to 0.02878, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 15/90\n",
      " - 12s - loss: 0.0400 - val_loss: 0.0242\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02878 to 0.02419, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 16/90\n",
      " - 11s - loss: 0.0314 - val_loss: 0.0255\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02419\n",
      "Epoch 17/90\n",
      " - 12s - loss: 0.0263 - val_loss: 0.0143\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02419 to 0.01433, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 18/90\n",
      " - 11s - loss: 0.0235 - val_loss: 0.0091\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01433 to 0.00912, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 19/90\n",
      " - 11s - loss: 0.0181 - val_loss: 0.0128\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00912\n",
      "Epoch 20/90\n",
      " - 12s - loss: 0.0258 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00912\n",
      "Epoch 21/90\n",
      " - 12s - loss: 0.0134 - val_loss: 0.0099\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00912\n",
      "Epoch 22/90\n",
      " - 11s - loss: 0.0139 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00912 to 0.00685, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 23/90\n",
      " - 11s - loss: 0.0155 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00685\n",
      "Epoch 24/90\n",
      " - 11s - loss: 0.0092 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00685 to 0.00344, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 25/90\n",
      " - 12s - loss: 0.0083 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00344\n",
      "Epoch 26/90\n",
      " - 11s - loss: 0.0100 - val_loss: 0.0062\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00344\n",
      "Epoch 27/90\n",
      " - 11s - loss: 0.0083 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00344\n",
      "Epoch 28/90\n",
      " - 11s - loss: 0.0100 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00344\n",
      "Epoch 29/90\n",
      " - 12s - loss: 0.0135 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00344 to 0.00286, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 30/90\n",
      " - 11s - loss: 0.0083 - val_loss: 0.0073\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00286\n",
      "Epoch 31/90\n",
      " - 11s - loss: 0.0045 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00286\n",
      "Epoch 32/90\n",
      " - 11s - loss: 0.0130 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00286\n",
      "Epoch 33/90\n",
      " - 11s - loss: 0.0049 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00286 to 0.00220, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 34/90\n",
      " - 11s - loss: 0.0034 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00220\n",
      "Epoch 35/90\n",
      " - 11s - loss: 0.0053 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00220\n",
      "Epoch 36/90\n",
      " - 11s - loss: 0.0042 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00220\n",
      "Epoch 37/90\n",
      " - 11s - loss: 0.0050 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00220\n",
      "Epoch 38/90\n",
      " - 11s - loss: 0.0113 - val_loss: 0.0141\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00220\n",
      "Epoch 39/90\n",
      " - 11s - loss: 0.0058 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00220 to 0.00149, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 40/90\n",
      " - 11s - loss: 0.0029 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00149 to 0.00148, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 41/90\n",
      " - 11s - loss: 0.0024 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00148 to 0.00122, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 42/90\n",
      " - 11s - loss: 0.0038 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00122\n",
      "Epoch 43/90\n",
      " - 11s - loss: 0.0071 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00122\n",
      "Epoch 44/90\n",
      " - 12s - loss: 0.0046 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00122\n",
      "Epoch 45/90\n",
      " - 12s - loss: 0.0036 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00122\n",
      "Epoch 46/90\n",
      " - 11s - loss: 0.0022 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00122\n",
      "Epoch 47/90\n",
      " - 11s - loss: 0.0030 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00122\n",
      "Epoch 48/90\n",
      " - 11s - loss: 0.0038 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00122\n",
      "Epoch 49/90\n",
      " - 11s - loss: 0.0095 - val_loss: 0.0084\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00122\n",
      "Epoch 50/90\n",
      " - 11s - loss: 0.0039 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00122\n",
      "Epoch 51/90\n",
      " - 11s - loss: 0.0028 - val_loss: 9.5684e-04\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00122 to 0.00096, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 52/90\n",
      " - 11s - loss: 0.0023 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00096\n",
      "Epoch 53/90\n",
      " - 11s - loss: 0.0020 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00096\n",
      "Epoch 54/90\n",
      " - 11s - loss: 0.0034 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00096\n",
      "Epoch 55/90\n",
      " - 11s - loss: 0.0036 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00096\n",
      "Epoch 56/90\n",
      " - 12s - loss: 0.0045 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00096\n",
      "Epoch 57/90\n",
      " - 11s - loss: 0.0032 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00096\n",
      "Epoch 58/90\n",
      " - 11s - loss: 0.0021 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00096\n",
      "Epoch 59/90\n",
      " - 11s - loss: 0.0019 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00096\n",
      "Epoch 60/90\n",
      " - 12s - loss: 0.0021 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00096\n",
      "Epoch 61/90\n",
      " - 11s - loss: 0.0048 - val_loss: 0.0090\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00096\n",
      "Epoch 62/90\n",
      " - 12s - loss: 0.0050 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00096\n",
      "Epoch 63/90\n",
      " - 11s - loss: 0.0023 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00096\n",
      "Epoch 64/90\n",
      " - 11s - loss: 0.0013 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00096\n",
      "Epoch 65/90\n",
      " - 11s - loss: 0.0012 - val_loss: 5.1695e-04\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00096 to 0.00052, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 66/90\n",
      " - 13s - loss: 0.0019 - val_loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00052\n",
      "Epoch 67/90\n",
      " - 11s - loss: 0.0014 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00052\n",
      "Epoch 68/90\n",
      " - 11s - loss: 0.0031 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00052\n",
      "Epoch 69/90\n",
      " - 13s - loss: 0.0036 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00052\n",
      "Epoch 70/90\n",
      " - 11s - loss: 0.0041 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00052\n",
      "Epoch 71/90\n",
      " - 12s - loss: 0.0040 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00052\n",
      "Epoch 72/90\n",
      " - 12s - loss: 0.0014 - val_loss: 7.4480e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00052\n",
      "Epoch 73/90\n",
      " - 13s - loss: 8.4343e-04 - val_loss: 5.5346e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00052\n",
      "Epoch 74/90\n",
      " - 12s - loss: 0.0014 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00052\n",
      "Epoch 75/90\n",
      " - 12s - loss: 0.0016 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00052\n",
      "Epoch 76/90\n",
      " - 13s - loss: 0.0017 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00052\n",
      "Epoch 77/90\n",
      " - 12s - loss: 0.0049 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00052\n",
      "Epoch 78/90\n",
      " - 11s - loss: 0.0027 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00052\n",
      "Epoch 79/90\n",
      " - 12s - loss: 0.0012 - val_loss: 3.8649e-04\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00052 to 0.00039, saving model to simple_seq2seq_checkpoint.h5\n",
      "Epoch 80/90\n",
      " - 11s - loss: 9.7603e-04 - val_loss: 7.4431e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00039\n",
      "Epoch 81/90\n",
      " - 11s - loss: 0.0020 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00039\n",
      "Epoch 82/90\n",
      " - 12s - loss: 0.0026 - val_loss: 6.3642e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00039\n",
      "Epoch 83/90\n",
      " - 11s - loss: 9.1068e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00039\n",
      "Epoch 84/90\n",
      " - 12s - loss: 0.0018 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00039\n",
      "Epoch 85/90\n",
      " - 12s - loss: 0.0012 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00039\n",
      "Epoch 86/90\n",
      " - 11s - loss: 0.0027 - val_loss: 9.2088e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00039\n",
      "Epoch 87/90\n",
      " - 11s - loss: 0.0078 - val_loss: 9.2262e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00039\n",
      "Epoch 88/90\n",
      " - 11s - loss: 0.0011 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00039\n",
      "Epoch 89/90\n",
      " - 11s - loss: 0.0012 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00039\n",
      "Epoch 90/90\n",
      " - 11s - loss: 0.0014 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Convergence plot for Simple Seq2Seq')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = simple_seq2seq.fit(X_train, np.expand_dims(Y_train, -1),\n",
    "                             validation_data=(X_val, np.expand_dims(Y_val, -1)),\n",
    "                             epochs=90, verbose=2, batch_size=32,\n",
    "                             callbacks=[best_model_cb])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], '--', label='validation')\n",
    "plt.ylabel('negative log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Convergence plot for Simple Seq2Seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the best model found on the validation set at the end of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_simple_seq2seq = load_model(best_model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how wel our model does on this fairly simple task. For anything reasonable we should train for mayb 50 epochs. To get rid of errors probably train for 150 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a raw prediction on the first sample of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achthundert ein und neunzig tausend einhundert und drei'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the numeric array this is provided (along with the expected target sequence) as the following padded input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 19, 50, 25, 49, 38, 50, 24, 15,  1, 12, 13,  5,  5,  4,  7]], dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_test_sequence = X_test[0:1]\n",
    "first_test_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe scroll up to find the table translating integer tokens into the actual numbers.\n",
    "\n",
    "Remember that the `_GO` (symbol indexed at `1`) separates the reversed source from the expected target sequence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_GO'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_shared_vocab[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the model prediction\n",
    "\n",
    "Let's feed a test sequence into the model. For each predicted output we get the argmax (to get the most likely symbol) and put that into our output.\n",
    "\n",
    "We also have to get rid of our padding and turn it back into a human friendly text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction shape: (1, 16, 62)\n",
      "prediction token ids: [ 0  0  0  0  0  0  0  0  0 12 13  5  5  4  7  2]\n",
      "predicted number: 891103\n",
      "test number: 891103\n"
     ]
    }
   ],
   "source": [
    "prediction = simple_seq2seq.predict(first_test_sequence)\n",
    "print(\"prediction shape:\", prediction.shape)\n",
    "\n",
    "# Let's use `argmax` to extract the predicted token ids at each step.\n",
    "# what does the last dimension stand for?\n",
    "predicted_token_ids = prediction[0].argmax(-1)\n",
    "print(\"prediction token ids:\", predicted_token_ids)\n",
    "\n",
    "# We can use the shared reverse vocabulary to map\n",
    "# the integer sequence back to the string representation of the tokens,\n",
    "# as well as removing Padding and EOS symbols\n",
    "predicted_numbers = [rev_shared_vocab[token_id] for token_id in predicted_token_ids\n",
    "                     if token_id not in (shared_vocab[PAD], shared_vocab[EOS])]\n",
    "print(\"predicted number:\", \"\".join(predicted_numbers))\n",
    "print(\"test number:\", num_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohhaayy!! But wait ...\n",
    "\n",
    "In the previous notebook cell we cheated because we gave the complete test sequence to the network as input. Along with the solution!\n",
    "\n",
    "To be more realistic we need to use the model in a setting where we do not provide any token of the expected translation as part of the input sequence: the model has to predict one token at a time starting only from the source sequence along with the `<GO>` special symbol. At each step, we append the newly predicted output token to the input sequence to predict the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True):\n",
    "    \"\"\"Greedy decoder that predicts one token at a time\"\"\"\n",
    "    # Initialise the list of input token IDs with the source sequence\n",
    "    source_tokens = tokenise(source_sequence, word_level=word_level_source)\n",
    "\n",
    "    input_ids = [shared_vocab.get(t, UNK) for t in reversed(source_tokens)]\n",
    "    input_ids += [shared_vocab[GO]]\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.empty(shape=(1, model.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    #print(input_array)\n",
    "    decoded_tokens = []\n",
    "    while len(input_ids) <= max_length:\n",
    "        # Vectorise the list of input tokens\n",
    "        # and use zero padding.\n",
    "        input_array.fill(shared_vocab[PAD])\n",
    "        # clever numpy indexing to set the last part of input_array\n",
    "        input_array[0, -len(input_ids):] = input_ids\n",
    "        \n",
    "        # Predict the next output: greedily decoding with argmax\n",
    "        # we pick the token with the highest probability at this step\n",
    "        next_token_id = model.predict(input_array)[0, -1].argmax()\n",
    "        \n",
    "        # Stop decoding if the network predicts \"end of sentence\":\n",
    "        if next_token_id == shared_vocab[EOS]:\n",
    "            #print(\"done\")\n",
    "            break\n",
    "            \n",
    "        # Otherwise use the reverse vocabulary to map the prediction\n",
    "        # back to the string space\n",
    "        decoded_tokens.append(rev_shared_vocab[next_token_id])\n",
    "        \n",
    "        # Append prediction to input sequence to predict the next token\n",
    "        input_ids.append(next_token_id)\n",
    "\n",
    "    separator = \" \" if word_level_target else \"\"\n",
    "    return separator.join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eins                 1\n",
      "zwei                 2\n",
      "drei                 3\n",
      "zwoelf               12\n",
      "fuenfzehn            15\n",
      "einhundert sieben und neunzig tausend neunhundert und sechs 197906\n",
      "siebenhundert drei und zwanzig tausend neunhundert sieben und zwanzig 147927\n",
      "zwei und vierzig     42\n",
      "dreihundert zwei und zwanzig 422\n"
     ]
    }
   ],
   "source": [
    "phrases = [\n",
    "    \"eins\",\n",
    "    \"zwei\",\n",
    "    \"drei\",\n",
    "    \"zwoelf\",\n",
    "    \"fuenfzehn\",\n",
    "    \"einhundert sieben und neunzig tausend neunhundert und sechs\",\n",
    "    \"siebenhundert drei und zwanzig tausend neunhundert sieben und zwanzig\",\n",
    "    \"zwei und vierzig\",\n",
    "    \"dreihundert zwei und zwanzig\"\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(#loaded_simple_seq2seq,\n",
    "        simple_seq2seq,\n",
    "                                   phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(20), translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train the model long enough (~100 epochs) you should be getting pretty good results. After only 15 epochs the network mostly just predicts the `<EOS>` token so it might look like it is not outputting anything/broken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Because we expect only one correct translation for a given source sequence, we can use phrase-level accuracy as a metric to quantify our model quality.\n",
    "\n",
    "Note that this is not the case for real translation models (e.g. from French to English on arbitrary sentences). Evaluation of a machine translation model in general is tricky. Automated evaluation can be done at the corpus level with the [BLEU score](https://en.wikipedia.org/wiki/BLEU) (bilingual evaluation understudy) given a large enough sample of correct translations provided by certified translators but its only a noisy proxy.\n",
    "\n",
    "The only good evaluation is to give a large enough sample of the model predictions on some test sentences to certified translators and ask them to give an evaluation (e.g. a score between 0 and 6, 0 for non-sensical and 6 for the hypothetical perfect translation). However in practice this is very costly to do, so you will almost always see BLEU scores quoted in papers.\n",
    "\n",
    "Fortunately we can just use phrase-level accuracy on a our very domain specific toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_accuracy(model, num_sequences, de_sequences, n_samples=500,\n",
    "                    decoder_func=greedy_translate):\n",
    "    correct = []\n",
    "    n_samples = len(num_sequences) if n_samples is None else n_samples\n",
    "    for i, num_seq, de_seq in zip(range(n_samples), num_sequences, de_sequences):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Decoding %d/%d\" % (i, n_samples))\n",
    "\n",
    "        predicted_seq = decoder_func(simple_seq2seq, de_seq,\n",
    "                                     shared_vocab, rev_shared_vocab,\n",
    "                                     word_level_target=False)\n",
    "        correct.append(num_seq == predicted_seq)\n",
    "        #if num_seq != predicted_seq:\n",
    "        #    print(num_seq, predicted_seq)\n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding 0/500\n",
      "Decoding 100/500\n",
      "Decoding 200/500\n",
      "Decoding 300/500\n",
      "Decoding 400/500\n",
      "Phrase-level train accuracy: 0.318\n"
     ]
    }
   ],
   "source": [
    "print(\"Phrase-level train accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_train, de_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding 0/500\n",
      "Decoding 100/500\n",
      "Decoding 200/500\n",
      "Decoding 300/500\n",
      "Decoding 400/500\n",
      "Phrase-level test accuracy: 0.360\n"
     ]
    }
   ],
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_test, de_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding 0/500\n",
      "Decoding 100/500\n",
      "Decoding 200/500\n",
      "Decoding 300/500\n",
      "Decoding 400/500\n",
      "Phrase-level test accuracy: 0.360\n"
     ]
    }
   ],
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(loaded_simple_seq2seq, num_test, de_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "seq2seq models are very powerful. You can translate from any kind of sequence to another sequence. This could be Numeric to German, German to Numeric, GitHub issue text to GitHub title, Python docstring to Python source code, ... or German to Klingon.\n",
    "\n",
    "There are a lot of details and tricks of the trade that we skipped over here. If you want to dive deeper follow some of the links below.\n",
    "\n",
    "\n",
    "# Going Further\n",
    "\n",
    "We only scratched the surface of sequence-to-sequence systems. To go further, read the initial [Sequence to Sequence paper](https://arxiv.org/abs/1409.3215) as well as the following developments, citing this work. Furthermore, here are a few pointers on how to go further if you're interested.\n",
    "\n",
    "### Improved model\n",
    "\n",
    "- implement [beam-search](https://en.wikipedia.org/wiki/Beam_search) instead of our greedy decoding.\n",
    "- Add multiple, larger GRU layers and more dropout regularization.\n",
    "- This should make it possible train a perfect translation model with a smaller amount of labeled samples. Try to train a seq2seq model with only 4000 training sequences or even fewer without overfitting.\n",
    "- You will need a GPU and more training time for that...\n",
    "\n",
    "### Reverse translation: Numeric to German\n",
    "\n",
    "- Build a model, with the same data from Numeric to German\n",
    "- The model should work well with the same kind of architecture as for German to Numeric\n",
    "\n",
    "\n",
    "### Separated Encoder-Decoder\n",
    "\n",
    "You may want to build a model with a separated encoder and decoder, to improve performance and to have more flexiblity with the architecture.\n",
    "\n",
    "With Keras, you can get access to the activation states of the LSTM cell by using:\n",
    "\n",
    "```python\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "```\n",
    "\n",
    "then you can reuse those states to initialize the state of the decoder cell:\n",
    "\n",
    "```python\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "```\n",
    "\n",
    "A full example of this architecture can be found at:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "TensorFlow also has some nice examples: https://www.tensorflow.org/tutorials/seq2seq.\n",
    "\n",
    "The opennmt project is worth looking at too: http://opennmt.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
