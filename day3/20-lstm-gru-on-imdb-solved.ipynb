{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Recurrent Neural Networks\n",
    "\n",
    "The goal of this notebook is to learn to use Recurrent Neural Networks for text classification.\n",
    "\n",
    "We will also create a convolutional neural network. These can train more quickly than recurrent networks because operations can be done in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB movie review dataset\n",
    "\n",
    "(same dataset as in the linear model + embeding exercise.)\n",
    "\n",
    "Fetch the dataset from http://ai.stanford.edu/~amaas/data/sentiment/ and un'tar it to\n",
    "a directory near to this notebook. I placed it in `../data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "class balance: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\", categories=['neg', 'pos'])\n",
    "\n",
    "text_trainval, y_trainval = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_trainval)))\n",
    "print(\"length of text_train: {}\".format(len(text_trainval)))\n",
    "print(\"class balance: {}\".format(np.bincount(y_trainval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly partition the text files in a training and test set while recording the target category of each file as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove some HTML and turn `bytes` into `str`\n",
    "text_trainval = [doc.replace(b\"<br />\", b\" \").decode() for doc in text_trainval]\n",
    "\n",
    "# Use train_test_split to split up your dataset\n",
    "texts_train, texts_test, target_train, target_test = train_test_split(\n",
    "    text_trainval, y_trainval, stratify=y_trainval, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_train[42]:\n",
      "I swear I could watch this movie every weekend of my life and never get sick of it! Every aspect of human emotion is captured so magically by the acting, the script, the direction, and the general feeling of this movie. It's been a long time since I saw a movie that actually made me choke from laughter, reflect from sadness, and feel each intended feeling that comes through in this most excellent work! We need MORE MOVIES like this!!! Mike Binder: are you listening???\n"
     ]
    }
   ],
   "source": [
    "# look at an example review, and some other sanity checks\n",
    "# just to make sure you properly loaded the data, splitting worked, etc\n",
    "print(\"text_train[42]:\\n{}\".format(text_trainval[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text for the (supervised) CBOW model\n",
    "\n",
    "We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n",
    "\n",
    "The following cells uses Keras to preprocess text:\n",
    "- using a tokenizer. You may use different tokenizers (from scikit-learn, spacy, custom Python function etc.). This converts the texts into sequences of indices representing the `20000` most frequent words\n",
    "- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`)\n",
    "- we convert the output classes as 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77916 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the length of the sequences and pad them if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (18750, 500)\n",
      "Shape of data test tensor: (6250, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# pad sequences with 0s\n",
    "X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_train.shape)\n",
    "print('Shape of data test tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (18750, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(target_train)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building more complex models\n",
    "\n",
    "- Use the model from the previous notebook as a template to build more complex models using:\n",
    "  - **1d convolution and 1d maxpooling**. Note that you will still need a `GloabalAveragePooling` or `Flatten` layer after the convolutions as the final `Dense` layer expects a fixed size input;\n",
    "  - **Recurrent neural networks using a LSTM** (you will need to **reduce sequence length before using the LSTM layer**).\n",
    "\n",
    "The model we are trying to build looks a bit like this:\n",
    "<img src=\"unrolled_rnn_one_output_2.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "### Bonuses\n",
    "- You may try different architectures with:\n",
    "  - more intermediate layers, combination of dense, conv, recurrent\n",
    "  - different recurrent network types (GRU, RNN)\n",
    "  - bidirectional LSTMs, checkout `day2/bi-directional-lstm-and-gru.py` which comes from [kaggle](https://www.kaggle.com/tunguz/bi-gru-lstm-cnn-poolings-fasttext). To run it you probably need a kaggle account. This implements a GRU and a LSTM, both of which are bidirectional.\n",
    "\n",
    "**Note**: The goal is to build working models rather than getting better test accuracy as this task is already very well solved by the simple model.  Build your model, and verify that they converge to OK results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, LSTM, GRU\n",
    "from keras.layers import MaxPooling1D, GlobalAveragePooling1D \n",
    "from keras.models import Model\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d convolutions to classify text\n",
    "\n",
    "We use 1D convolutions to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# A 1D convolution with 128 output channels\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "# MaxPool divides the length of the sequence by 5\n",
    "x = MaxPooling1D(5)(x)\n",
    "# A 1D convolution with 64 output channels\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "# MaxPool divides the length of the sequence by 5\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16875 samples, validate on 1875 samples\n",
      "Epoch 1/2\n",
      "16875/16875 [==============================] - 43s 3ms/step - loss: 0.4224 - acc: 0.7770 - val_loss: 0.2819 - val_acc: 0.8891\n",
      "Epoch 2/2\n",
      "16875/16875 [==============================] - 43s 3ms/step - loss: 0.1611 - acc: 0.9420 - val_loss: 0.2899 - val_acc: 0.8955\n",
      "Test accuracy: 0.88496\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.1,\n",
    "          epochs=2, batch_size=32)\n",
    "\n",
    "output_test = model.predict(X_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model with convolutions\n",
    "\n",
    "We need to shorten the sequence a bit using convolutions+maxpooling to speed up the training\n",
    "process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# 1D convolution with 64 output channels\n",
    "x = Conv1D(64, 5)(embedded_sequences)\n",
    "\n",
    "# MaxPool divides the length of the sequence by 5: this is helpful\n",
    "# to train the LSTM layer on shorter sequences. The LSTM layer\n",
    "# can be very expensive to train on longer sequences.\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(64, 5)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "\n",
    "# LSTM layer with a hidden size of 64\n",
    "x = LSTM(64)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# You will get large speedups with these models by using a GPU\n",
    "# The model might take a lot of time to converge, and even more\n",
    "# if you add dropout (needed to prevent overfitting)\n",
    "# I'd recommend to get a kaggle.com account and use the free\n",
    "# GPUs they provide or to try out http://colab.research.google.com/\n",
    "# which also provides free GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16875 samples, validate on 1875 samples\n",
      "Epoch 1/2\n",
      "16875/16875 [==============================] - 37s 2ms/step - loss: 0.3847 - acc: 0.8106 - val_loss: 0.2903 - val_acc: 0.8869\n",
      "Epoch 2/2\n",
      "16875/16875 [==============================] - 36s 2ms/step - loss: 0.1512 - acc: 0.9439 - val_loss: 0.3192 - val_acc: 0.8736\n",
      "Test accuracy: 0.8736\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.1,\n",
    "          epochs=2, batch_size=32)\n",
    "\n",
    "output_test = model.predict(X_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
