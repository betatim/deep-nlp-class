{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing text for Machine-learning\n",
    "\n",
    "A long document made up of words is not a good representation for computers.\n",
    "\n",
    "We need to convert the text into a better representation. Here we will look at\n",
    "some baseline methods that work very well in practice. They also form the basis\n",
    "of more complicated ideas used later on. Strong baselines are important in a world\n",
    "of deep learning. You have to be able to demosntrate that the additional complexity,\n",
    "reduced explainability, and additional technical debt from using deep learning is\n",
    "worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The Uber driver behind the wheel of an autonomous car that hit \"\n",
    "             \"and killed a pedestrian in Arizona could have avoided the collision \"\n",
    "             \"if she had not been distracted, according to police investigating \"\n",
    "             \"the incident.\",\n",
    "             \"An avoidability analysis by police in Tempe, Arizona following March's\"\n",
    "             \" crash suggested that Rafaela Vasquez, Uber's safety driver, may have \"\n",
    "             \"been watching the online video service Hulu in the car.\",\n",
    "             \"The death of 49-year-old Elaine Herzberg is believed to be the first \"\n",
    "             \"time an autonomous car has killed a bystander, prompting a series of \"\n",
    "             \"investigations into what happened.\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bag of words representation of the above sentences\n",
    "# Build an `dict` that can map from a word to an integer\n",
    "# use all sentences to build this dictionary\n",
    "# Use it to convert each sentence into a sequence of integers\n",
    "# Encode each integer in a one-hot fashion (check the keras notebook if you\n",
    "# need a refresher)\n",
    "# sum the individual one-hot encoded words in a sentence to produce your\n",
    "# bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that you can undo your bag of words\n",
    "# create an inverse index (dictionary) that maps from\n",
    "# integers back to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what fraction of entries are not zero in X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X>0).sum(axis=1) / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly small vocabulary and just three sentences but already most of the entries in the vector are zeros. Note that the (cosine) distance between two vectors for individual words carries no semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ready made tools for this\n",
    "\n",
    "scikit-learn has tools to do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thead/anaconda/envs/deep-nlp-class/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fce4e9378033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ... use an instance of CountVectorizer() to vectorise your sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vect' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ... use an instance of CountVectorizer() to vectorise your sentences\n",
    "vect = CountVectorizer()\n",
    "\n",
    "print(sorted(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the learnt mapping to convert sentences to bag of words\n",
    "# CountVectorizer is a transformer. It has a `transform()`\n",
    "# method that you can use after fitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the vector representation back to \"words\"\n",
    "# there is a helper to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a difference between what `CountVectorizer` says how many features (words)\n",
    "# there are and your manual code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify movies from their reviews\n",
    "\n",
    "Fetch the dataset from http://ai.stanford.edu/~amaas/data/sentiment/ and un'tar it to\n",
    "a directory near to this notebook. I placed my copy in `../data/`, however it is too\n",
    "large to distribute with the repository so you will have to get your own copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CountVectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b4058f0fc9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreviews_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/aclImdb/train/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CountVectorizer'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\", categories=['neg', 'pos'])\n",
    "\n",
    "text_trainval, y_trainval = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_trainval)))\n",
    "print(\"length of text_train: {}\".format(len(text_trainval)))\n",
    "print(\"class balance: {}\".format(np.bincount(y_trainval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classes: {}\".format(set(y_trainval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at an example review\n",
    "print(\"text_train[42]:\\n{}\".format(text_trainval[42].decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the review texts!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# sort out some HTML tags. and make a list of all training and\n",
    "# validation reviews\n",
    "text_trainval = [doc.replace(b\"<br />\", b\" \").decode() for doc in text_trainval]\n",
    "\n",
    "# Split the data into training and validation data.\n",
    "# Use CountVectorizer() to fit and transform the training\n",
    "# data and transform the validation data\n",
    "# ... your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out some of the words\n",
    "feature_names = vect.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[30000:30010])\n",
    "print(feature_names[::3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a logistic regression model to the data and measure the performance\n",
    "# of the model. We now have a baseline we need to improve on.\n",
    "# What score do you achieve on which dataset? Did you split into training\n",
    "# and testing datasets?\n",
    "# \n",
    "# Once you have your model running tune the regularisation strength `C`\n",
    "# using `RandomSearchCV` or `LogisticRegressionCV` which is more efficient\n",
    "# Take a look at the scikit-learn documentation for each of the\n",
    "# classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which words correlate with a good and a bad review.\n",
    "# You can inspect the weights of the linear model by looking\n",
    "# at the coefs_ property of your LogisticRegression instance\n",
    "# the sign and magnitude of a coefficient tells you how much\n",
    "# and in which direction the model uses each feature for\n",
    "# its decisions\n",
    "# Check the documentation to learn about all the properties:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#\n",
    "# do they seem sensible? Could you explain why a review is\n",
    "# getting a high or a low predicted sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code here ...\n",
    "# checkout `plt.bar()` to make a plot of the twenty most\n",
    "# negative and twenty most positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "* investigate how to configure the vectorizer to exclude stop words\n",
    "* should you fix the spelling of misspelt words?\n",
    "* only include words that appear more than N times?\n",
    "* only include the M most frequent words?\n",
    "* how few examples do you need to achieve a \"good\" performance? Study the score as a function of the number of reviews you train on\n",
    "* a bag of words does not know anything about the order of words,\n",
    "  can you construct bi-grams (pairs of words) and improve the\n",
    "  performance? Check the documentation for `CountVectorizer`\n",
    "* construct a logistic regression model in keras and use that instead\n",
    "  of using scikit-learn's implementation\n",
    "\n",
    "\n",
    "# Term frequency, inverse document frequency\n",
    "\n",
    "Instead of counting how often a word appears we can also use TfIdf.\n",
    "\n",
    "Check out the scikit-learn documentation http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Pipelines are extremly useful tools\n",
    "# http://scikit-learn.org/stable/modules/pipeline.html\n",
    "text_classifier = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3, max_df=0.8, ngram_range=(1, 2)),\n",
    "    LogisticRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_classifier.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier.score(text_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline results\n",
    "\n",
    "With about 20s of computer time and a few minutes of work we are at 88% accuracy.\n",
    "\n",
    "We have not really tuned this baseline yet, it is possible that some investment in \n",
    "finding better hyperparamters will improve the baseline further. Try it out if you have time\n",
    "by looking at `RandomizedSearchCV` http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
