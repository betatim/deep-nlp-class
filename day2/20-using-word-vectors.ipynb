{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors - word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors are a method that allow you to represent words as dense vectors when you input them to your model. This is an alternative to the sparse vectors we have been creating so far with `CountVectorizer` or `TfIdfVectorizer`.\n",
    "\n",
    "\n",
    "## Why word vectors?\n",
    "\n",
    "One hot encoded words don't let us learn anything about how similar two words are. Say we have a vocabulary of three: green, red, dog. Assume red is encoded as `[1, 0, 0]`, green as `[0, 1, 0]` and dog as `[0, 0, 1]`. Any distance metric you choose will not be able to show you that red and green are more similar to each other than to dog.\n",
    "\n",
    "With word vectors we could represent red as `[0.1, 0.2]`, green as `[0.15, 0.21]` and dog as `[0.8, 0.87]`. Now the distance (cosine or euclidean) between red and green is small, and the distance to dog is large.\n",
    "\n",
    "This notebook is about building some intuition about vector spaces and answering the crucial question of how do we come up with a new vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example: colours\n",
    "\n",
    "Let's start with a vector space that we are all familiar with: the vector space of RGB colours.\n",
    "\n",
    "Colours can be represented as vectors with three dimensions: red, green, and blue. We can use these vectors to answer questions like: which colours are similar? What's the most likely colour name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colours, what's the name of those colours' \"average\"?\n",
    "\n",
    "We will be using this [colour data](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json) from the [xkcd colour survey](https://blog.xkcd.com/2010/05/03/color-survey-results/). The data relates a colour name to the RGB value associated with that colour. [Here's a page that shows what the colors look like](https://xkcd.com/color/rgb/). Download the colour data and put it in a directory near to this notebook (I used `../data/`).\n",
    "\n",
    "---\n",
    "\n",
    "Aside on colour spaces:\n",
    "* If you're interested in perceptually accurate colour math in Python, consider using the [colormath library](http://python-colormath.readthedocs.io/en/latest/).\n",
    "* A fantastic video about how to design colourmaps for plots: https://www.youtube.com/watch?v=xAoljeRJ3lU - how people spent a lot of time to design matplotlibs new default colourmap.\n",
    "* [\"Make it pop\"](https://predictablynoisy.com/makeitpop-intro), if you can't use Jet as a colourmap anymore, just distort your data instead! This piece is not seriously suggesting you distort the data, but it explains how you could.\n",
    "\n",
    "---\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "colour_data = json.loads(open(\"../data/xkcd.json\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts colours from hex format (`#1a2b3c`) to a tuple of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_int(s):\n",
    "    s = s.lstrip(\"#\")\n",
    "    return np.array([int(s[:2], 16), int(s[2:4], 16), int(s[4:6], 16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = dict()\n",
    "for item in colour_data['colors']:\n",
    "    colours[item[\"color\"]] = hex_to_int(item[\"hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([110, 117,  14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colours['olive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([229,   0,   0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colours['red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colours['black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 255, 255])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colours['cyan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math\n",
    "\n",
    "A useful helper for computing the distance between two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance(coord1, coord2):\n",
    "    # do you know what these two lines do?\n",
    "    coord1 = np.asarray(coord1)\n",
    "    coord2 = np.asarray(coord2)\n",
    "    return np.linalg.norm(coord1 - coord2)\n",
    "\n",
    "assert np.allclose(distance([10, 1], [5, 2]), 5.0990195135927845)\n",
    "distance([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `meanv` function takes a list of vectors and finds their mean or average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meanv(coords):\n",
    "    coords = np.atleast_2d(coords)\n",
    "    return np.sum(coords, axis=0) / float(coords.shape[0])\n",
    "    \n",
    "\n",
    "m = meanv([[0, 1], [2, 2], [4, 3]])\n",
    "assert np.allclose(m, [2.0, 2.0]), m\n",
    "meanv([[0, 1], [2, 2], [4, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a test, the following cell shows that the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(colours['red'], colours['green']) > distance(colours['red'], colours['pink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the closest item\n",
    "\n",
    "The `closest()` function below is a helper to find the `n` nearest neighbours for a query colour.\n",
    "\n",
    "> Note: Calculating \"nearest neighbours\" like this is fine for the examples in this notebook, but plesae don't ever use it. There are grown up versions of this:\n",
    "* SciPy's [kdtree](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html)\n",
    "* [KDtree in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html)\n",
    "* [Annoy](https://pypi.python.org/pypi/annoy) by Spotify for approximate nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own `closest` to make sure you understand\n",
    "# how this works. The below verison makes use of several\n",
    "# advanced ideas from Python.\n",
    "# Bonus:\n",
    "# Can you write a version that uses numpy constructs?\n",
    "# This would likely be much faster (to be sure benchmark it).\n",
    "def closest(space, coord, n=10):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out, we can find the ten colours closest to \"green\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['green',\n",
       " 'kelly green',\n",
       " 'irish green',\n",
       " 'true green',\n",
       " 'emerald green',\n",
       " 'kelley green',\n",
       " 'grass green',\n",
       " 'vibrant green',\n",
       " 'grassy green',\n",
       " 'emerald']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colours, colours['green'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the ten colours closest to (150, 60, 150):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['burnt umber',\n",
       " 'red brown',\n",
       " 'brownish red',\n",
       " 'russet',\n",
       " 'brick',\n",
       " 'rust',\n",
       " 'auburn',\n",
       " 'brown red',\n",
       " 'rust brown',\n",
       " 'warm brown']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colours, [150, 60, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='background: #963c14;\n",
       "                    height: 100px;\n",
       "                    width: 100px' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def int_to_hex(colour):\n",
    "    r, g, b = colour\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(r,g,b)\n",
    "\n",
    "HTML(\"\"\"<div style='background: {};\n",
    "                    height: 100px;\n",
    "                    width: 100px' />\"\"\".format(int_to_hex([150, 60, 20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you call this colour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colour magic\n",
    "\n",
    "The interesting thing about representing words as vectors is that the vector operations we defined earlier appear to operate on language the same way they operate on numbers. For example, if we find the word closest to the vector resulting from subtracting \"red\" from \"purple,\" we get a series of \"blue\" colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cobalt blue',\n",
       " 'royal blue',\n",
       " 'darkish blue',\n",
       " 'true blue',\n",
       " 'royal',\n",
       " 'prussian blue',\n",
       " 'dark royal blue',\n",
       " 'deep blue',\n",
       " 'marine blue',\n",
       " 'deep sea blue']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colours, colours['purple'] - colours['red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches our intuition about RGB colors, which is that purple is a combination of red and blue. Take away the red, and blue is all you have left.\n",
    "\n",
    "You can do something similar with addition. What's blue plus green?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bright turquoise',\n",
       " 'bright light blue',\n",
       " 'bright aqua',\n",
       " 'cyan',\n",
       " 'neon blue',\n",
       " 'aqua blue',\n",
       " 'bright cyan',\n",
       " 'bright sky blue',\n",
       " 'aqua',\n",
       " 'bright teal']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colours, colours['blue'] + colours['green'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, something like turquoise or cyan! What if we find the average of black and white? Predictably, we get grey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medium grey',\n",
       " 'purple grey',\n",
       " 'steel grey',\n",
       " 'battleship grey',\n",
       " 'grey purple',\n",
       " 'purplish grey',\n",
       " 'greyish purple',\n",
       " 'steel',\n",
       " 'warm grey',\n",
       " 'green grey']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the average of black and white: medium grey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use colour vectors to reason about relationships between colours. In the cell below, finding the difference between \"pink\" and \"red\" then adding it to \"blue\" seems to give us a list of colours that are to blue what pink is to red (a slightly lighter, less saturated shade):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neon blue',\n",
       " 'bright sky blue',\n",
       " 'bright light blue',\n",
       " 'cyan',\n",
       " 'bright cyan',\n",
       " 'bright turquoise',\n",
       " 'clear blue',\n",
       " 'azure',\n",
       " 'dodger blue',\n",
       " 'lightish blue']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an analogy: pink is to red as X is to blue\n",
    "pink_to_red = colours['pink'] - colours['red']\n",
    "closest(colours, pink_to_red + colours['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are fairly simple from a mathematical perspective but they are demonstrating that it is possible to use math to reason about how people use language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics\n",
    "\n",
    "In the previous section, the examples are interesting because of a simple fact: colours that we think of as similar are \"closer\" to each other in RGB vector space. In our colour vector space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing. They are also, for many purposes, *functionally identical*. Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it is probably also okay to show them results for, say,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mauve trousers\n",
      "dusty rose trousers\n",
      "dusky rose trousers\n",
      "brownish pink trousers\n",
      "old pink trousers\n",
      "reddish grey trousers\n",
      "dirty pink trousers\n",
      "old rose trousers\n",
      "light plum trousers\n",
      "ugly pink trousers\n"
     ]
    }
   ],
   "source": [
    "for cname in closest(colours, colours['mauve']):\n",
    "    print(cname + \" trousers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property? How do we build such a vector space for all words in a language? We learn them!\n",
    "\n",
    "To understand how that works, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), which states that:\n",
    "\n",
    "    Linguistic items with similar distributions have similar meanings.\n",
    "    \n",
    "What's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n",
    "\n",
    "    It was really cold yesterday.\n",
    "    It will be really warm today, though.\n",
    "    It'll be really hot tomorrow!\n",
    "    Will it be really cool Tuesday?\n",
    "    \n",
    "According to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n",
    "\n",
    "In other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors from predicting contexts\n",
    "\n",
    "For us to be able to learn such \"contexts\" from data we need to be able to do this as an unsupervised task. We will need a lot of text to learn about rare words and their contexts. There is nearly an infinite amount of text data on the internet (wikipedia, news, books, ...), but it is not labelled. Or is it?\n",
    "\n",
    "Actually every sentence provides many examples of the context in which a word is being used.\n",
    "We can take each word, plus some context words around it and construct a task that tries to predict the central word from the context. Or tries to predict the context from the central word. This is an auxiliary task that we are using to learn our continuous representation (or word vectors)!\n",
    "\n",
    "\n",
    "### Skip-Gram models\n",
    "\n",
    "Task: given a word, predict surrounding words.\n",
    "This is a supervised task, but we do not need a \"labelled\" dataset. Each document yields many examples for free! We are not interested in performance for this task, just want to learn representations.\n",
    "\n",
    "(This idea of constructing a \"self-supervised\" task to learn embeddings will come back again later. For example if you want to learn an embedding for documents.)\n",
    "\n",
    "This is based on the idea that neural networks are first and foremost feature transformers. All the intermediate layers of a neural network transform features from one representation to a new one that is (slightly) more useful. We will input our words as one-hot encoded vectors that form a bag of words and as part of solving the task of predicting the surrounding words the neural network will learn a better representation!\n",
    "\n",
    "\n",
    "<img src=\"cbow_skipgram_1.png\" style=\"height: 300px;\" />\n",
    "\n",
    "In fact this will be a very simple neural network. Just one layer. In this figure `V` is the size of the vocabulary, `N` the dimensionality of the embedding space, and `C` the number of context words. The input and output are represented as `V` dimensional one-hot encoded vectors. It is the intermediate layer of size `N` that will hold our continous, dense vector embedding.\n",
    "\n",
    "### Implementations\n",
    "\n",
    "* [Gensim](https://radimrehurek.com/gensim/)\n",
    "* spaCy\n",
    "* Word2vec\n",
    "* Tensorflow\n",
    "* Mostly not something you train yourself, just use pre-made word vectors\n",
    "\n",
    "\n",
    "### GloVe vectors\n",
    "\n",
    "In practive you do not have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors in spaCy\n",
    "\n",
    "Okay, let's have some fun with real word vectors. We're going to use the GloVe vectors that come with spaCy to creatively analyze and manipulate the text of *Frankenstein*.\n",
    "\n",
    "> The default model in spacy does not have word vectors, you have to install\n",
    "> at least the \"medium\" sized model with `python -m spacy download en_core_web_md`\n",
    "\n",
    "First, make sure you've got `spacy` imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Download the text of Frankenstein from http://www.gutenberg.org/ebooks/84\n",
    "> I placed my copy in `../data/`.\n",
    "\n",
    "The following cell loads the language model and parses the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy's `en_core_web_md` model into avariable called `nlp`\n",
    "# process all of Frankenstein through spacy and place the result\n",
    "# in a variable called `doc`\n",
    "# we want to use `en_core_web_md` because it contains good word\n",
    "# vectors for english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a list of unique words (or tokens) in the text, as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the words in the text file that aren't punctuation\n",
    "# and store them in `tokens`\n",
    "tokens = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file',\n",
       " 'farewell',\n",
       " 'image',\n",
       " 'equal',\n",
       " 'startled',\n",
       " 'clear',\n",
       " 'task',\n",
       " 'files',\n",
       " 'voyages',\n",
       " 'besieged']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the vector of any word in spaCy's vocabulary using the `vocab` attribute, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3758e-01,  4.8054e-01, -6.6233e-03, -1.7448e-01,  4.4043e-01,\n",
       "       -3.1442e-01,  5.3397e-01,  4.6729e-01,  1.0159e-01,  2.7244e+00,\n",
       "       -7.6771e-01,  1.7797e-01,  7.5324e-01,  2.2318e-01,  2.2972e-01,\n",
       "        1.3957e-02,  1.1651e-01,  1.5249e-01,  5.4813e-01,  1.5076e-02,\n",
       "       -2.6553e-01, -6.2747e-01,  2.1325e-01, -2.2996e-01, -1.8529e-01,\n",
       "        5.5864e-01, -8.4094e-01,  7.0990e-01, -2.9615e-01,  2.5878e-01,\n",
       "       -8.8265e-02, -3.4606e-01, -2.5755e-01, -8.2202e-02,  6.2264e-02,\n",
       "        1.8445e-01, -1.5417e-04, -2.3137e-01, -5.9262e-03,  3.9931e-01,\n",
       "        1.4636e-01,  2.2448e-01,  6.0828e-01,  6.3412e-01, -3.2131e-01,\n",
       "       -6.8973e-01, -1.6703e-01,  5.3806e-01, -4.4930e-01,  6.4328e-02,\n",
       "       -2.3113e-01,  2.0405e-01,  1.1714e-01, -7.9628e-01, -9.4335e-02,\n",
       "       -1.1125e-01,  2.9469e-01,  3.9459e-01,  3.8567e-01,  2.5055e-01,\n",
       "       -5.9500e-03,  8.5024e-01,  2.1191e-02,  1.9802e-01, -2.0702e-01,\n",
       "        2.5742e-01,  2.9258e-01,  4.8071e-01, -2.7758e-01,  3.7001e-01,\n",
       "        4.5153e-03, -3.2577e-01,  3.3899e-01, -9.4189e-02, -3.1678e-01,\n",
       "        8.5872e-02, -8.2441e-02,  2.5764e-01,  7.1869e-01,  2.1872e-02,\n",
       "       -3.7896e-01, -1.5745e-01,  2.1325e-01,  2.2310e-01, -1.7459e-01,\n",
       "        1.7333e-02, -6.8089e-02,  8.6216e-01,  3.3767e-01, -3.1811e-01,\n",
       "       -4.6660e-01, -1.2068e-01, -3.6997e-01, -3.9240e-01,  2.2843e-01,\n",
       "       -6.4458e-01, -2.5317e-01,  2.3299e-01,  4.4993e-01, -1.6620e-01,\n",
       "       -2.6254e-02, -3.0508e-01,  1.2899e-01, -8.1603e-02, -1.2730e-01,\n",
       "       -2.8213e-01,  5.0937e-01,  1.0035e-01,  3.0005e-01, -1.4234e-01,\n",
       "       -5.1414e-01, -3.8344e-01, -9.4929e-01, -1.4279e-01, -4.1396e-01,\n",
       "        3.5399e-01, -9.9060e-02,  1.8087e-01, -5.5557e-01, -2.2607e-01,\n",
       "        3.9099e-01, -8.7826e-02, -3.6965e-01,  4.0530e-03,  1.2119e-01,\n",
       "       -3.2074e-01,  1.0286e-01,  3.2345e-01, -1.1859e-01,  5.3256e-01,\n",
       "        2.7154e-01, -6.2959e-01, -3.5240e-02, -2.3130e-01,  1.2831e-01,\n",
       "       -7.0530e-01,  1.3315e-01, -1.4291e-02,  1.5936e-01, -5.2866e-02,\n",
       "       -2.1793e+00,  3.2779e-01,  3.1632e-01,  4.3216e-01,  2.4968e-01,\n",
       "       -1.5000e-01,  6.3198e-01, -2.9670e-02, -4.9345e-02,  2.8474e-01,\n",
       "       -3.9967e-01,  1.3212e-01, -2.7061e-01, -1.4497e-01, -5.0933e-01,\n",
       "       -1.8130e-01,  4.7501e-02, -2.5818e-01, -2.8367e-02, -5.5086e-01,\n",
       "        1.5848e-01,  5.9384e-02,  3.7468e-01, -4.4623e-01, -4.8684e-02,\n",
       "       -1.5969e-01,  3.6383e-02,  2.3168e-01, -1.3996e-01,  1.2695e-01,\n",
       "       -4.1933e-01,  3.0987e-01, -4.1837e-01, -1.3507e-01,  1.6802e-01,\n",
       "       -3.1311e-01, -5.8379e-01,  4.9051e-02,  1.7413e-01, -4.9133e-02,\n",
       "       -1.4301e-01,  1.2402e-01,  9.5811e-02, -1.4529e-02, -5.7770e-01,\n",
       "       -4.6492e-01, -2.4233e-01,  5.8521e-01,  1.8539e-01, -6.4840e-01,\n",
       "        1.2564e-01,  1.7128e-01, -7.0255e-02,  6.3289e-02, -4.0216e-01,\n",
       "        1.2127e-01,  3.9636e-01, -1.3503e-01,  4.6509e-01,  1.7811e-01,\n",
       "        1.2850e+00,  2.1105e-01,  5.2596e-01,  1.9149e-01, -5.3835e-01,\n",
       "       -3.1260e-01,  2.4893e-01, -3.1155e-01, -1.8059e-01,  1.9540e-01,\n",
       "        7.0958e-02,  4.1886e-01,  5.5161e-01,  1.2242e-01, -3.0601e-01,\n",
       "        5.7418e-01,  1.0361e-01, -1.6243e-01, -9.6178e-01,  2.7180e-01,\n",
       "       -1.9421e-01, -1.0030e-01, -5.8728e-01, -4.1919e-01, -1.0874e-01,\n",
       "        4.1510e-01,  8.3939e-02,  2.3021e-01,  1.0636e+00,  7.8745e-01,\n",
       "       -4.7059e-01,  5.8792e-01, -8.0760e-01,  3.2797e-02, -2.9936e-01,\n",
       "       -4.0029e-01,  2.4289e-01,  4.2235e-01, -3.2213e-01,  7.7461e-01,\n",
       "        2.0879e-01, -3.0621e-01,  3.7728e-01, -3.3787e-01, -7.9398e-01,\n",
       "        2.1572e-01,  3.1555e-02,  7.1406e-02, -1.2431e-01,  7.2566e-01,\n",
       "        7.7986e-01, -3.4411e-01,  1.5192e-01, -4.7010e-01,  2.9341e-01,\n",
       "       -3.2222e-01, -4.7519e-01, -5.2294e-01, -2.7548e-01, -5.3277e-01,\n",
       "        6.6997e-01,  5.4175e-01,  2.6160e-01,  4.3066e-01, -5.7042e-03,\n",
       "        3.9321e-01, -4.7794e-01, -2.1968e-01, -4.4973e-01, -1.0916e-01,\n",
       "       -1.6458e-01,  5.6755e-01,  3.7410e-01,  2.6301e-02, -2.4768e-01,\n",
       "       -4.4299e-02,  1.3045e-01,  5.8490e-02, -4.7333e-01, -4.7797e-01,\n",
       "       -7.3105e-01, -5.1379e-01, -1.4802e+00,  1.1776e-01, -1.6133e-01,\n",
       "       -1.5983e-01, -1.3594e-01, -1.7140e-01,  9.7515e-02,  1.0947e-01,\n",
       "        1.1836e-01, -4.1326e-01, -4.7250e-01, -3.3535e-01,  1.6876e-01,\n",
       "       -9.2063e-02, -7.8021e-02, -4.9248e-01,  5.1232e-01, -1.8974e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['football'].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity and finding closest neighbors\n",
    "\n",
    "The cell below defines a function `cosine()`, which returns the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of two vectors. Cosine similarity is another way of determining how similar two vectors are, which is more suited to high-dimensional spaces. [See the Encyclopedia of Distances for more information and even more ways of determining vector similarity.](http://www.uco.es/users/ma1fegan/Comunes/asignaturas/vision/Encyclopedia-of-distances-2009.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function `cosine(vec1, vec2)` that computes the\n",
    "# cosine distance between two vectors. Use numpy constructs\n",
    "# from `numpy.linalg` where needed.\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that the cosine similarity between `football` and `barbeque` is larger than the similarity between `trousers` and `octopus`.\n",
    "\n",
    "The vectors are working how we expect them to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('football'), vec('barbeque')) > cosine(vec('trousers'), vec('octopus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function `spacy_closest(tokens, vector, n=10)` that finds\n",
    "# the `n` tokens closest to `vector` by checking against the vectors\n",
    "# of the tokens in the list `tokens`.\n",
    "\n",
    "def spacy_closest(list_of_all_tokens, vector, n=10):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, get a list of synonyms, or words closest in meaning (or distribution, depending on how you look at it), to any arbitrary word in spaCy's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coach',\n",
       " 'league',\n",
       " 'college',\n",
       " 'game',\n",
       " 'junior',\n",
       " 'sport',\n",
       " 'school',\n",
       " 'leagues',\n",
       " 'play',\n",
       " 'ball']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the closest equivalent of basketball?\n",
    "# remember these tokens came from Frankenstein!\n",
    "spacy_closest(tokens, vec(\"basketball\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does basketball even come up in Frankenstein?\n",
    "\"basketball\" in tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with spaCy, Frankenstein, and vector arithmetic\n",
    "\n",
    "Now we can start doing vector arithmetic and finding the closest words to the resulting vectors. For example, what word is closest to the halfway point between day and night?\n",
    "\n",
    "Remember that `tokens` only contains words that spacy reconises and are present in the text of _Frankenstein_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['night',\n",
       " 'Night',\n",
       " 'Day',\n",
       " 'day',\n",
       " 'evening',\n",
       " 'Morning',\n",
       " 'morning',\n",
       " 'afternoon',\n",
       " 'nights',\n",
       " 'week']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halfway between day and night\n",
    "spacy_closest(tokens, meanv([vec(\"day\"), vec(\"night\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations of `night` and `day` are still closest, but after that we get words like `evening` and `morning`, which are indeed halfway between day and night!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are close to \"wine\" and \"cheese\" in *Frankenstein*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'vineyards',\n",
       " 'drink',\n",
       " 'moonshine',\n",
       " 'taste',\n",
       " 'draught',\n",
       " 'tasted',\n",
       " 'cheese',\n",
       " 'dinner',\n",
       " 'delicious']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"wine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheese',\n",
       " 'bread',\n",
       " 'soup',\n",
       " 'milk',\n",
       " 'roasted',\n",
       " 'delicious',\n",
       " 'Vegetables',\n",
       " 'vegetables',\n",
       " 'loaf',\n",
       " 'softened']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"cheese\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you subtract \"alcohol\" from \"wine\" and find the closest words to the resulting vector, you're left with simply a lovely dinner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'vineyards',\n",
       " 'graceful',\n",
       " 'exquisite',\n",
       " 'marvellous',\n",
       " 'magnificent',\n",
       " 'delightful',\n",
       " 'dinner',\n",
       " 'lovely',\n",
       " 'leaved']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"wine\") - vec(\"alcohol\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the closest words to \"water\"? What about adding \"frozen\" to \"water\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'waters',\n",
       " 'Salt',\n",
       " 'dry',\n",
       " 'Ocean',\n",
       " 'ocean',\n",
       " 'heat',\n",
       " 'pebble',\n",
       " 'sands',\n",
       " 'Sea']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"water\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'Frozen',\n",
       " 'frozen',\n",
       " 'cold',\n",
       " 'Cold',\n",
       " 'chilly',\n",
       " 'ices',\n",
       " 'ice',\n",
       " 'freezing',\n",
       " 'Salt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"water\") + vec(\"frozen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do analogies! For example, the words most similar to \"grass\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grass',\n",
       " 'sod',\n",
       " 'herbage',\n",
       " 'foliage',\n",
       " 'trees',\n",
       " 'gardener',\n",
       " 'garden',\n",
       " 'brambles',\n",
       " 'bushes',\n",
       " 'pebble']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take the difference of \"blue\" and \"sky\" and add it to grass, you get the analogous word (\"green\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grass',\n",
       " 'sod',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'red',\n",
       " 'pink',\n",
       " 'blue',\n",
       " 'leaf',\n",
       " 'herbage',\n",
       " 'white']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogy: blue is to sky as X is to grass\n",
    "blue_to_sky = vec(\"blue\") - vec(\"sky\")\n",
    "spacy_closest(tokens, blue_to_sky + vec(\"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you are now convinced that the vector space in which these words are embedded is useful for finding similar words. The definition of \"similar\" roughly corresponds to what humans think of as similar words as well, but it is not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about computing the similarity of sentences? The simlest thing to do to get the vector for a sentence is to average its component vectors, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the sentence in our text file that is closest in \"meaning\" to an arbitrary input sentence. First, we'll get the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of sentences from a spaCy parse and compares them to an input sentence, sorting them by cosine similarity.\n",
    "\n",
    "Here a function `spacy_closest_sent(space, input_sent, n=10)` that finds the `n`\n",
    "sentences closest to `input_sent` in a collection of sentences in `space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sent2vec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the sentences in *Frankenstein* closest in meaning to \"My favourite food is strawberry ice cream.\" (Extra linebreaks are present because we didn't strip them out when we originally read in the source text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vegetables in the gardens, the milk and cheese that I saw\n",
      "placed at the windows of some of the cottages, allured my appetite.\n",
      "---\n",
      "I greedily devoured the\n",
      "remnants of the shepherd’s breakfast, which consisted of bread, cheese,\n",
      "milk, and wine; the latter, however, I did not like.  \n",
      "---\n",
      "I\n",
      "had first, however, provided for my sustenance for that day by a loaf\n",
      "of coarse bread, which I purloined, and a cup with which I could drink\n",
      "more conveniently than from my hand of the pure water which flowed by\n",
      "my retreat.  \n",
      "---\n",
      "My food is not\n",
      "that of man; I do not destroy the lamb and the kid to glut my appetite;\n",
      "acorns and berries afford me sufficient nourishment.  \n",
      "---\n",
      "For some\n",
      "time I sat upon the rock that overlooks the sea of ice.  \n",
      "---\n",
      "Their nourishment\n",
      "consisted entirely of the vegetables of their garden and the milk of\n",
      "one cow, which gave very little during the winter, when its masters\n",
      "could scarcely procure food to support it.  \n",
      "---\n",
      "we\n",
      "saw some dogs drawing a sledge, with a man in it, across the ice.\n",
      "---\n",
      "When night came again I\n",
      "found, with pleasure, that the fire gave light as well as heat and that\n",
      "the discovery of this element was useful to me in my food, for I found\n",
      "some of the offals that the travellers had left had been roasted, and\n",
      "tasted much more savoury than the berries I gathered from the trees.  \n",
      "---\n",
      "When they had retired to rest, if there was any\n",
      "moon or the night was star-light, I went into the woods and collected\n",
      "my own food and fuel for the cottage.  \n",
      "---\n",
      "“One night during my accustomed visit to the neighbouring wood where I\n",
      "collected my own food and brought home firing for my protectors, I found on\n",
      "the ground a leathern portmanteau containing several articles of dress and\n",
      "some books.  \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for sent in spacy_closest_sent(sentences, \"My favourite food is strawberry ice cream.\"):\n",
    "    print(sent.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In production\n",
    "\n",
    "spaCy (unsurprisingly) has all this built in. For a production system you would not code your own cosine similarity function and please do not use our toy \"nearest neighbour\" functions but a kdtree or other efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: My favourite food is strawberry ice cream.\n",
      "Other: My favourite food is strawberry ice cream.\n",
      "Similarity: 1.0\n",
      "Other: My favourite dessert is chocolate cake.\n",
      "Similarity: 0.9139987084798473\n",
      "Other: My favourite hobby is running marathons.\n",
      "Similarity: 0.7208354513782261\n",
      "Base: My favourite dessert is chocolate cake.\n",
      "Other: My favourite food is strawberry ice cream.\n",
      "Similarity: 0.9139987084798473\n",
      "Other: My favourite dessert is chocolate cake.\n",
      "Similarity: 1.0\n",
      "Other: My favourite hobby is running marathons.\n",
      "Similarity: 0.7291110517011878\n",
      "Base: My favourite hobby is running marathons.\n",
      "Other: My favourite food is strawberry ice cream.\n",
      "Similarity: 0.7208354513782261\n",
      "Other: My favourite dessert is chocolate cake.\n",
      "Similarity: 0.7291110517011878\n",
      "Other: My favourite hobby is running marathons.\n",
      "Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "strawberry = nlp(\"My favourite food is strawberry ice cream.\")\n",
    "chocolate = nlp(\"My favourite dessert is chocolate cake.\")\n",
    "running = nlp(\"My favourite hobby is running marathons.\")\n",
    "\n",
    "for base in (strawberry, chocolate, running):\n",
    "    print(\"Base:\", base)\n",
    "    for other in (strawberry, chocolate, running):\n",
    "        print(\"Other:\", other)\n",
    "        print(\"Similarity:\", base.similarity(other))\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the vector of a sentence spaCy averages the vectors of the words in a sentence. This works surprisingly well for short sentences. After about 10-15 words the vectors become too noisy to be useful, so this is not a good option for (long) documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy's builtin methods to compute the similarity between sentences and\n",
    "# create a `spacy_closest_sent(space, input_str, n=10)` based on that.\n",
    "\n",
    "def spacy_native_closest_sent(space, input_str, n=10):\n",
    "    input = nlp(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: input.similarity(x),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vegetables in the gardens, the milk and cheese that I saw\n",
      "placed at the windows of some of the cottages, allured my appetite.\n",
      "---\n",
      "I greedily devoured the\n",
      "remnants of the shepherd’s breakfast, which consisted of bread, cheese,\n",
      "milk, and wine; the latter, however, I did not like.  \n",
      "---\n",
      "I\n",
      "had first, however, provided for my sustenance for that day by a loaf\n",
      "of coarse bread, which I purloined, and a cup with which I could drink\n",
      "more conveniently than from my hand of the pure water which flowed by\n",
      "my retreat.  \n",
      "---\n",
      "My food is not\n",
      "that of man; I do not destroy the lamb and the kid to glut my appetite;\n",
      "acorns and berries afford me sufficient nourishment.  \n",
      "---\n",
      "For some\n",
      "time I sat upon the rock that overlooks the sea of ice.  \n",
      "---\n",
      "Their nourishment\n",
      "consisted entirely of the vegetables of their garden and the milk of\n",
      "one cow, which gave very little during the winter, when its masters\n",
      "could scarcely procure food to support it.  \n",
      "---\n",
      "we\n",
      "saw some dogs drawing a sledge, with a man in it, across the ice.\n",
      "---\n",
      "When night came again I\n",
      "found, with pleasure, that the fire gave light as well as heat and that\n",
      "the discovery of this element was useful to me in my food, for I found\n",
      "some of the offals that the travellers had left had been roasted, and\n",
      "tasted much more savoury than the berries I gathered from the trees.  \n",
      "---\n",
      "When they had retired to rest, if there was any\n",
      "moon or the night was star-light, I went into the woods and collected\n",
      "my own food and fuel for the cottage.  \n",
      "---\n",
      "“One night during my accustomed visit to the neighbouring wood where I\n",
      "collected my own food and brought home firing for my protectors, I found on\n",
      "the ground a leathern portmanteau containing several articles of dress and\n",
      "some books.  \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for sent in spacy_native_closest_sent(sentences,\n",
    "                                      \"My favourite food is strawberry ice cream.\"):\n",
    "    print(sent.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Use spacy's vocab to create the list of tokens together with an efficient\n",
    "lookup method.\n",
    "\n",
    "* Can you replicate some of the word-math from the slides?\n",
    "* What is the most similar word to \"king\" - \"man\" + \"woman\"?\n",
    "* Can you fix spelling mistakes using word vectors?\n",
    "* Unfortunately the only German model available for spacy does not\n",
    "  include word vectors. If you have time, download the fastText vectors\n",
    "  for German and see how things go.\n",
    "\n",
    "## Further resources\n",
    "\n",
    "* [Word2vec](https://en.wikipedia.org/wiki/Word2vec) is another procedure for producing word vectors which uses a predictive approach rather than a context-counting approach. [This paper](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf) compares and contrasts the two approaches. (Spoiler: not much difference.)\n",
    "* If you want to train your own word vectors on a particular corpus, the popular Python library [gensim](https://radimrehurek.com/gensim/) has an implementation of Word2Vec that is relatively easy to use. [There's a good tutorial here.](https://rare-technologies.com/word2vec-tutorial/)\n",
    "* When you're working with vector spaces with high dimensionality and millions of vectors, iterating through your entire space calculating cosine similarities is slow. Use [Annoy](https://pypi.python.org/pypi/annoy) to make the lookup faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
