{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word vectors\n",
    "\n",
    "In the last notebook we saw several applications of word and sentence vectors. In this notebook we will train our own.\n",
    "\n",
    "For this we will use the gensim library.\n",
    "\n",
    "Quoting Wikipedia:\n",
    "\n",
    "> Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.\n",
    "\n",
    "LGPL licensed, https://github.com/RaRe-Technologies/gensim.\n",
    "\n",
    "Documentation for the `Word2Vec` class: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "To get going, we will need to have a set of documents to train our word2vec model. In theory, a document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book. In NLP parlance a collection or set of documents is often referred to as a corpus.\n",
    "\n",
    "We need to create a list of sentences to feed to the `Word2Vec` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `conda install -c conda-forge gensim` to install this\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model can take a moment\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and chewing through the whole of Frankenstein as well\n",
    "doc = nlp(open(\"../data/84-0.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see that the sentence boundary detection is not\n",
    "# perfect in spacy, especially at the beginning of the\n",
    "# book which contains lots of strangely formatted text.\n",
    "for n, sentence in enumerate(doc.sents):\n",
    "    # skip the first 40 \"sentences\"\n",
    "    # disable to see the weird ones\n",
    "    if n < 40:\n",
    "        continue\n",
    "    # maybe preprocessing the text like this helps\n",
    "    print(\" \".join(w.lower_.strip() for w in sentence))\n",
    "    print(\"-\" * 80)\n",
    "    if n > 40 + 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec training needs a generator of sentences. Let's write one that skips over the first part of the book, and then applies some normalisation to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems simpler to use a generator than create a whole class\n",
    "# as is shown in the gensim documentation. YMMV.\n",
    "def sentences(document):\n",
    "    for n, sentence in enumerate(document.sents):\n",
    "        if n < 40:\n",
    "            continue\n",
    "        # maybe preprocessing the text like this helps\n",
    "        yield [w.lower_.strip() for w in sentence if w.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your generator creates sentences\n",
    "# one sentence per iteration, one sentence is a list of words\n",
    "next(sentences(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 dimensional vectors are probably enough for such a small text\n",
    "# experiment a bit with what works best\n",
    "w2v = Word2Vec(size=20, min_count=3, iter=10)\n",
    "w2v.build_vocab(sentences(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.train(sentences(doc),\n",
    "          total_examples=w2v.corpus_count,\n",
    "          epochs=w2v.iter\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the vocabulary\n",
    "w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(\"violence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(\"cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector for \"milk\"\n",
    "w2v['milk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors for \"milk\" and \"cabin\"\n",
    "w2v[['milk', 'cabin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary size is very small compared to any of the pre-traiend vectors. These vectors are \"tuned\" to this particular text but probably not as useful as using generic word vectors from Glove.\n",
    "\n",
    "One thing to notice is that if you have a lot of specific jargon in your documents you might improve your performance by training a specialised set of word vectors. Because for words out of the vocabulary (like misspelt ones) you have no vector to assign. Often people simply set them to zero or initialise them randomly.\n",
    "\n",
    "\n",
    "## Using word vectors for movie reviews\n",
    "Let's compare using self-trained word vectors to simple TfIdf on the movie sentiment task. Use what you learnt above to train (small) word vectors on the IMBD dataset we used previously.\n",
    "\n",
    "To train word vectors we need to:\n",
    "* load all the individual reviews and chunk them into sentences\n",
    "* feed sentences to our `Word2vec` model\n",
    "* train the model\n",
    "* inspect word vectors (for sanity checking)\n",
    "\n",
    "After training the vectors and checking that they are somewhat sensible try\n",
    "and use them as input features for a logistic regression model instead of TfIdf\n",
    "or the `CountVectorizer` that we used before in `10-tfidf.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\", categories=['neg', 'pos'])\n",
    "\n",
    "text_trainval, y_trainval = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_trainval)))\n",
    "print(\"length of text_train: {}\".format(len(text_trainval)))\n",
    "print(\"class balance: {}\".format(np.bincount(y_trainval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "text_trainval = [doc.replace(b\"<br />\", b\" \") for doc in text_trainval]\n",
    "\n",
    "text_train, text_val, y_train, y_val = train_test_split(\n",
    "    text_trainval, y_trainval, stratify=y_trainval, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "# turn off features from spacy that we don't need\n",
    "tokenizer.remove_pipe(\"ner\")\n",
    "tokenizer.remove_pipe(\"tagger\")\n",
    "tokenizer.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "def movie_sentences(text):\n",
    "    for sample in text:\n",
    "        doc = tokenizer(sample.decode())\n",
    "        for sentence in doc.sents:\n",
    "            # maybe preprocessing the text like this helps\n",
    "            yield [w.lower_.strip() for w in sentence if w.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compare the speed of the tokenizer to a full spacy model\n",
    "# that performs NER etc\n",
    "# probably want to use the %%time magic\n",
    "next(movie_sentences(text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this step can take quite some time :(\n",
    "# there is a pickle of all the sentences in the\n",
    "# repository which you can just load instead of having\n",
    "# to run this yourself.\n",
    "all_movie_sentences = list(movie_sentences(text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load sentence list\n",
    "with open(\"all_movie_sentences.pkl\", \"rb\") as f:\n",
    "    all_movie_sentences = pickle.load(f)\n",
    "\n",
    "# store sentence list\n",
    "#with open(\"all_movie_sentences.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(all_movie_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "movie_w2v = Word2Vec(size=50, workers=5)\n",
    "# no RAM? Use this slower verison\n",
    "#movie_w2v.build_vocab(movie_sentences(text_train))\n",
    "movie_w2v.build_vocab(all_movie_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# no RAM? Use this slower verison\n",
    "#movie_w2v.train(movie_sentences(text_train),\n",
    "#                total_examples=movie_w2v.corpus_count,\n",
    "#                epochs=movie_w2v.iter\n",
    "#                )\n",
    "movie_w2v.train(all_movie_sentences,\n",
    "                total_examples=movie_w2v.corpus_count,\n",
    "                epochs=movie_w2v.iter\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you get more specific synonyms than before\n",
    "# compare to what spacy would find as similar words\n",
    "# to movie\n",
    "movie_w2v.wv.most_similar(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading the model is easy\n",
    "movie_w2v.save(\"movie_w2v_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_movie_w2v = Word2Vec.load(\"movie_w2v_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_movie_w2v.wv.most_similar(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_movie_w2v.wv.most_similar(\"batman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use word vectors as input to a logistic regression\n",
    "# Let's see if we can improve on our baseline for movie reviews by using\n",
    "# our own word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_w2v = CountVectorizer(vocabulary=loaded_movie_w2v.wv.index2word)\n",
    "vect_w2v.fit(text_train)\n",
    "docs = vect_w2v.inverse_transform(vect_w2v.transform(text_train))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average of the word vectors in a review to represent the whole document\n",
    "# place your training data in `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what should the shape of the training data in X_train be?\n",
    "# What size is your embedding?\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this compuares the average word vectors for the validation dataset\n",
    "val_docs = vect_w2v.inverse_transform(vect_w2v.transform(text_val))\n",
    "\n",
    "X_val = np.vstack([np.mean(loaded_movie_w2v[doc], axis=0) for doc in val_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_w2v = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "lr_w2v.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w2v.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you improve this by preprocessing the words that are given to the Word2Vec model\n",
    "# For example by removing stop words?\n",
    "# Check out the documentation for `CountVectorizer` to see if you can find the\n",
    "# stopword list used by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
