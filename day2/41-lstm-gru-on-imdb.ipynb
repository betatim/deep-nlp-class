{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Neural Networks\n",
    "\n",
    "The goal of this notebook is to learn to use Neural Networks for text classification.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Train a shallow model which learns embeddings\n",
    "- Download pre-trained embeddings from Glove\n",
    "- Use these pre-trained embeddings\n",
    "\n",
    "Keep in mind:\n",
    "- Deep Learning can be better on text classification that simpler ML techniques, but only on very large datasets and well designed/tuned models.\n",
    "- We won't be using the most efficient (in terms of computing) techniques, as Keras is good for prototyping but rather inefficient for training small embedding models on text.\n",
    "- The following projects can replicate similar word embedding models much more efficiently: [word2vec](https://github.com/dav/word2vec) and [gensim's word2vec](https://radimrehurek.com/gensim/models/word2vec.html)   (self-supervised learning only), [fastText](https://github.com/facebookresearch/fastText) (both supervised and self-supervised learning). However hard to see inside. We will use them tomorrow.\n",
    "- Plain shallow sparse TF-IDF bigrams features without any embedding and Logistic Regression or Multinomial Naive Bayes is often competitive in small to medium datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB movie review dataset\n",
    "\n",
    "(same dataset as in the TfIdf exercise.)\n",
    "\n",
    "Fetch the dataset from http://ai.stanford.edu/~amaas/data/sentiment/ and un'tar it to\n",
    "a directory near to this notebook. I placed it in `../data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "class balance: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../data/aclImdb/train/\", categories=['neg', 'pos'])\n",
    "\n",
    "text_trainval, y_trainval = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_trainval)))\n",
    "print(\"length of text_train: {}\".format(len(text_trainval)))\n",
    "print(\"class balance: {}\".format(np.bincount(y_trainval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly partition the text files in a training and test set while recording the target category of each file as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove some HTML and turn `bytes` into `str`\n",
    "text_trainval = [doc.replace(b\"<br />\", b\" \").decode() for doc in text_trainval]\n",
    "\n",
    "# Use train_test_split to split up your dataset\n",
    "texts_train, texts_test, target_train, target_test = train_test_split(\n",
    "    text_trainval, y_trainval, stratify=y_trainval, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_train[42]:\n",
      "I swear I could watch this movie every weekend of my life and never get sick of it! Every aspect of human emotion is captured so magically by the acting, the script, the direction, and the general feeling of this movie. It's been a long time since I saw a movie that actually made me choke from laughter, reflect from sadness, and feel each intended feeling that comes through in this most excellent work! We need MORE MOVIES like this!!! Mike Binder: are you listening???\n"
     ]
    }
   ],
   "source": [
    "# look at an example review, and some other sanity checks\n",
    "# just to make sure you properly loaded the data, splitting worked, etc\n",
    "print(\"text_train[42]:\\n{}\".format(text_trainval[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first baseline model\n",
    "\n",
    "For simple topic classification problems, one should always try a simple method first. In this case a good baseline is extracting TF-IDF normalized bag of bi-grams features and then use a simple linear classifier such as logistic regression.\n",
    "\n",
    "It's a very efficient method and should give us a strong baseline to compare our deep learning method against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# create a pipeline from the TfidfVectorizer and a LogisticRegression\n",
    "# fit and score the model. Make a note of the amount of CPU time.\n",
    "text_classifier = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3, max_df=0.8, ngram_range=(1, 2)),\n",
    "    LogisticRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 1.46 s, total: 22 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = text_classifier.fit(texts_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88495999999999997"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.score(texts_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should reach a score of around 88%. It's unlikely that we can significantly beat this baseline with a more complex deep learning based model. However let's try to reach a comparable level of accuracy with an `Embedding`s-based model for teaching purpose.\n",
    "\n",
    "To create a really competitive benchmark you should tune the hyper-parameters of the `TfidfVectorizer` and `LogisticRegression`. Come back to this later if you have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text for the (supervised) CBOW model\n",
    "\n",
    "We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n",
    "\n",
    "The following cells uses Keras to preprocess text:\n",
    "- using a tokenizer. You may use different tokenizers (from scikit-learn, spacy, custom Python function etc.). This converts the texts into sequences of indices representing the `20000` most frequent words\n",
    "- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`)\n",
    "- we convert the output classes as 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77916 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized sequences are converted to list of token ids (with an integer code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275, 42, 39, 84, 9, 24, 31, 1595, 1059, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message (without formatting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 77916)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_index), len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"maybe it's just because i have an intense fear of hospitals and medical stuff but this one got under my skin pardon the pun this piece is brave not afraid to go over the top and as satisfying as they come in terms of revenge movies not only did i find myself feeling lots of hatred for the and lots of sympathy towards the i felt myself cringe and feel of disgust at certain which is really a rare and delightful thing for a somewhat jaded horror viewer like myself some parts are very of hellraiser but come off as tribute rather than imitation it's a heavy handed piece that does not offer the viewer much to consider but i enjoy being assaulted by a film once and awhile this piece brings it and doesn't i liked this one a lot do not watch whilst eating pudding\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([index_to_word[i] for i in sequences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the tokenized sequences. The next task is dealing with the fact that each review has a different length. We will have to decide a maximum length and then convert all reviews accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length: 226.4\n",
      "max length: 1788\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It almost looks like there was a hand made cut off at 1000, but some longer reviews got through.\n",
    "\n",
    "Let's zoom on the distribution of regular sized reviews. The vast majority of the reviews have less than 500 symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([l for l in seq_lens if l < 500], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's truncate and pad all the sequences to 500 symbols to build the training set. Could you find a more quantative way to decide what sequence length to keep? Maybe look at the 95% quantile using `numpy`.\n",
    "\n",
    "Use `pad_sequences` from `keras.preprocessing.sequence` to do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (18750, 500)\n",
      "Shape of data test tensor: (6250, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# pad sequences with 0s\n",
    "X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_train.shape)\n",
    "print('Shape of data test tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (18750, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(target_train)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple supervised CBOW model in Keras\n",
    "\n",
    "The following computes a very simple model, as described in [fastText](https://github.com/facebookresearch/fastText):\n",
    "\n",
    "<img src=\"fasttext.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "- Build an embedding layer mapping each word to a vector representation\n",
    "- Compute the vector representation (`Embedding`) of all words in each sequence and average them (`GlobalAveragePooling1D`)\n",
    "  - start with an embedding size of 50\n",
    "- Add a `Dense` layer to output 2 classes (+ softmax)\n",
    "- connect everything together in a keras `Model`.\n",
    "\n",
    "Once you have a working model (debug using a small dataset of 10 samples maybe), `fit` it, and score it on the test dataset.\n",
    "\n",
    "Some more questiosn to ask yourself:\n",
    "How many epochs should you use (investigate `validation_split` argument to `fit? How much data do you need? What happens if you switch optimizer? How big/small can you make the embedding dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(np.unique(y_train))\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.01), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16875 samples, validate on 1875 samples\n",
      "Epoch 1/10\n",
      "16875/16875 [==============================] - 10s 576us/step - loss: 0.3774 - acc: 0.8332 - val_loss: 0.2954 - val_acc: 0.8928\n",
      "Epoch 2/10\n",
      "16875/16875 [==============================] - 9s 562us/step - loss: 0.1750 - acc: 0.9354 - val_loss: 0.3413 - val_acc: 0.8768\n",
      "Epoch 3/10\n",
      "16875/16875 [==============================] - 10s 563us/step - loss: 0.1062 - acc: 0.9624 - val_loss: 0.3722 - val_acc: 0.8875\n",
      "Epoch 4/10\n",
      "16875/16875 [==============================] - 10s 593us/step - loss: 0.0673 - acc: 0.9764 - val_loss: 0.4382 - val_acc: 0.8875\n",
      "Epoch 5/10\n",
      "16875/16875 [==============================] - 10s 563us/step - loss: 0.0405 - acc: 0.9872 - val_loss: 0.5512 - val_acc: 0.8789\n",
      "Epoch 6/10\n",
      "16875/16875 [==============================] - 9s 562us/step - loss: 0.0306 - acc: 0.9892 - val_loss: 0.5940 - val_acc: 0.8795\n",
      "Epoch 7/10\n",
      "16875/16875 [==============================] - 10s 564us/step - loss: 0.0231 - acc: 0.9918 - val_loss: 0.6745 - val_acc: 0.8763\n",
      "Epoch 8/10\n",
      "16875/16875 [==============================] - 9s 560us/step - loss: 0.0160 - acc: 0.9946 - val_loss: 0.9613 - val_acc: 0.8405\n",
      "Epoch 9/10\n",
      "16875/16875 [==============================] - 10s 569us/step - loss: 0.0136 - acc: 0.9953 - val_loss: 0.8473 - val_acc: 0.8693\n",
      "Epoch 10/10\n",
      "16875/16875 [==============================] - 10s 565us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 0.8295 - val_acc: 0.8752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1255c0f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercices**\n",
    "\n",
    "- Compute model accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "output_test = model.predict(X_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building more complex models\n",
    "\n",
    "**Exercise**\n",
    "- From the previous template, build more complex models using:\n",
    "  - **1d convolution and 1d maxpooling**. Note that you will still need a GloabalAveragePooling or Flatten after the convolutions as the final `Dense` layer expects a fixed size input;\n",
    "  - **Recurrent neural networks through LSTM** (you will need to **reduce sequence length before using the LSTM layer**).\n",
    "\n",
    "  \n",
    "<img src=\"images/unrolled_rnn_one_output_2.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "**Bonus**\n",
    "- You may try different architectures with:\n",
    "  - more intermediate layers, combination of dense, conv, recurrent\n",
    "  - different recurrent (GRU, RNN)\n",
    "  - bidirectional LSTMs\n",
    "\n",
    "**Note**: The goal is to build working models rather than getting better test accuracy as this task is already very well solved by the simple model.  Build your model, and verify that they converge to OK results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, LSTM, GRU\n",
    "from keras.layers import MaxPooling1D, GlobalAveragePooling1D \n",
    "from keras.models import Model\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(target_names)\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# TODO\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/conv1d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=5, batch_size=32)\n",
    "\n",
    "output_test = model.predict(x_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained embeddings\n",
    "\n",
    "The file `glove100K.100d.txt` is an extract of [Glove](http://nlp.stanford.edu/projects/glove/) Vectors, that were trained on english Wikipedia 2014 + Gigaword 5 (6B tokens).\n",
    "\n",
    "We extracted the `100 000` most frequent words. They have a dimension of `100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_vectors = []\n",
    "with open('glove100K.100d.txt', 'rb') as f:\n",
    "    word_idx = 0\n",
    "    for line in f:\n",
    "        values = line.decode('utf-8').split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = word_idx\n",
    "        embeddings_vectors.append(vector)\n",
    "        word_idx = word_idx + 1\n",
    "\n",
    "inv_index = {v: k for k, v in embeddings_index.items()}\n",
    "print(\"found %d different words in the file\" % word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all embeddings in a large numpy array\n",
    "glove_embeddings = np.vstack(embeddings_vectors)\n",
    "glove_norms = np.linalg.norm(glove_embeddings, axis=-1, keepdims=True)\n",
    "glove_embeddings_normed = glove_embeddings / glove_norms\n",
    "print(glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings[idx]\n",
    "\n",
    "    \n",
    "def get_normed_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings_normed[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_emb(\"computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most similar words\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Build a function to find most similar words, given a word as query:\n",
    "- lookup the vector for the query word in the Glove index;\n",
    "- compute the cosine similarity between a word embedding and all other words;\n",
    "- display the top 10 most similar words.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "Change your function so that it takes multiple words as input (by averaging them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/most_similar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"pitt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"jolie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the future better than tarot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(get_normed_emb('aniston'), get_normed_emb('pitt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(get_normed_emb('jolie'), get_normed_emb('pitt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus: yangtze is a chinese river\n",
    "most_similar([\"river\", \"chinese\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying vectors with  t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "word_emb_tsne = TSNE(perplexity=30).fit_transform(glove_embeddings_normed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "axis = plt.gca()\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.scatter(word_emb_tsne[:, 0], word_emb_tsne[:, 1], marker=\".\", s=1)\n",
    "\n",
    "for idx in range(1000):\n",
    "    plt.annotate(inv_index[idx],\n",
    "                 xy=(word_emb_tsne[idx, 0], word_emb_tsne[idx, 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "plt.savefig(\"tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained embeddings in our model\n",
    "\n",
    "We want to use these pre-trained embeddings for transfer learning. This process is rather similar than transfer learning in image recognition: the features learnt on words might help us bootstrap the learning process, and increase performance if we don't have enough training data.\n",
    "- We initialize embedding matrix from the model with Glove embeddings:\n",
    " - take all words from our 20 Newsgroup vocabulary (`MAX_NB_WORDS = 20000`), and look up their Glove embedding \n",
    " - place the Glove embedding at the corresponding index in the matrix\n",
    " - if the word is not in the Glove vocabulary, we only place zeros in the matrix\n",
    "- We may fix these embeddings or fine-tune them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words_in_matrix = 0\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = get_emb(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
    "        \n",
    "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a layer with pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_layer = Embedding(\n",
    "    MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model with pre-trained Embeddings\n",
    "\n",
    "Average word embeddings pre-trained with Glove / Word2Vec usually works surprisingly well. However, when averaging more than `10-15` words, the resulting vector becomes too noisy and classification performance is degraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "\n",
    "# We don't want to fine-tune embeddings\n",
    "model.layers[1].trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.01), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "- On this type of task, using pre-trained embeddings can degrade results as we train much less parameters and we average a large number pre-trained embeddings.\n",
    "\n",
    "- Pre-trained embeddings followed by global averaging prevents overfitting but can also cause some underfitting.\n",
    "\n",
    "- Using convolutions / LSTM should help counter the underfitting effect.\n",
    "\n",
    "- It is also advisable to treat separately pre-trained embeddings and words out of vocabulary.\n",
    "\n",
    "Pre-trained embeddings can be very useful when the training set is small and the individual text documents to classify are short: in this case there might be a single very important word in a test document that drives the label. If that word has never been seen in the training set but some synonyms were seen, the semantic similarity captured by the embedding will allow the model to generalized out of the restricted training set vocabulary.\n",
    "\n",
    "We did not observe this effect here because the document are long enough so that guessing the topic can be done redundantly. Shortening the documents to make the task more difficult could possibly highlight this benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reality check\n",
    "\n",
    "On small/medium datasets, simpler classification methods usually perform better, and are much more efficient to compute. Here are two resources to go further:\n",
    "- Naive Bayes approach, using scikit-learn http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "- Alec Radford (OpenAI) gave a very interesting presentation, showing that you need a VERY large dataset to have real gains from GRU/LSTM in text classification https://www.slideshare.net/odsc/alec-radfordodsc-presentation\n",
    "\n",
    "However, when looking at features, one can see that classification using simple methods isn't very robust, and won't generalize well to slightly different domains (e.g. forum posts => emails)\n",
    "\n",
    "Note: Implementation in Keras for text is very slow due to python overhead and lack of hashing techniques. The fastText implementation https://github.com/facebookresearch/fasttext is much, much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "- Compare pre-trained embeddings vs specifically trained embeddings\n",
    "- Train your own wordvectors in any language using [gensim's word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "- Check [Keras Examples](https://github.com/fchollet/keras/tree/master/examples) on `imdb` sentiment analysis\n",
    "- Install fastText (Linux or macOS only, use the Linux VM if under Windows) and give it a try on the classification example in its repository.\n",
    "- Today, the **state-of-the-art text classification** can be achieved by **transfer learning from a language model** instead of using traditional word embeddings. See for instance: FitLaM, Fine-tuned Language Models for Text Classification https://arxiv.org/abs/1801.06146. The second notebook introduces how to train such a language model from unlabeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
